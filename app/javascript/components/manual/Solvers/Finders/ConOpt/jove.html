<!DOCTYPE HTML PUBLIC "-//W4C//DTD HTML 4.2//EN">
<html>
<head>
   <title>Solver JOVE</title>
   <style>
       tr { vertical-align: top; }
   </style>
</head>
<body text="#000066">

<h1 align=center>Solver JOVE</h1>

<P>The solver JOVE is a sequential unconstrained optimization technique. The 
theoretical basis for this technique is thoroughly treated in <i>Nonlinear 
Programming: Sequential Unconstrained Minimization Techniques</i>, 
Fiacco, A.V. and McCormick, G.P. (Wiley 1968).  In this method, 
a new objective function is constructed from the original one by adding 
penalty functions involving the constraints. Then this augmented objective 
is optimized by an unconstrained optimization technique.</P>

<A NAME="FIND_JOVE" ID="FIND_JOVE"></a>
<P>The general form of the FIND statement for JOVE is:<BR><BR>

<blockquote>
<B>FIND</B> <i>unknowns</i><B> IN</B> <i>model</i><B> BY JOVE</B> 
&nbsp;&nbsp;{<B>(</B><i>controller</i><B>)</B>}<B></B><BR>
&nbsp;&nbsp;{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B></b>}  
{<B>WITH</B>|<B>AND</B>} <B>UPPER</b> <i>ceiling</i><B></B>}  <BR>
&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B></B>} {<B>WITH FLAG</B> <i>signal</i><b></b>}<BR>
&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B></B>} {<B>MATCHING</B> <i>equalities</i><B></B>}<BR>
&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i>
</blockquote>

where the optional clauses may appear in any order.  The common elements, 
<i>unknowns</i>, <i>model</i>, <i>controller</i>, and <i>auxiliaries</i>, 
are exactly as described for the general FIND statement in Section 
2.2.2.  The eleMents <i>floor</i> and <i>ceiling</i> are lists containing 
constant constraints. If present, these lists must correspond in size 
to the <i>unknowns</i> list. The FLAG parameter <i>signal</i> permits 
JOVE to signal the condition of problem solution, as explained  in 
Section 2.2.5.  The constraints if any, are specified in the HOLDING 
and MATCHING clauses.  Each prescribes a list of variables whose values 
represent the constraints.  The final objective clause identifies 
the function to be optimized and the desired type of extremum.</P>

<P>When JOVE is invoked by execution of the FIND statement, it activates 
derivative evaluation to compute gradients and Hessians with respect 
to the <i>unknowns</i>.</P>

<H3>Example</h3>

&nbsp;&nbsp;&nbsp;&nbsp;Find x,y which minimizes f = (x-2)<SUP>2</SUP> + (y-1)<SUP>2</SUP><BR>

&nbsp;&nbsp;&nbsp;&nbsp;Subject to:<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g(x,y) = 1 -x<SUP>2</SUP>/4 - y<M^>2 &ge; 0<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h(x,y) = e<SUP>xy</SUP> - x - 2  = 0

&nbsp;&nbsp;&nbsp;&nbsp;starting from the feasible point (-1,0)

<P>Program:</P>
<PRE>
      PROBLEM SIMPLE 
        COMMON/EQS/X,Y,G,H,F 
        X=-1 : Y=0  ! Initial estimates 
        FIND X,Y IN MODF BY JOVE 
          HOLDING G AND MATCHING H 
          TO MINIMIZE F 
      END

      MODEL MODF 
        COMMON/EQS/X,Y,G,H,F 
        G=1-X**2/4 - Y**2 
        H=EXP(X*Y) -X -2 
        F = (X-2)**2 + (Y-1)**2 
      END    
</PRE>


<P><B><i>The Solution Process </i></B> -  The essential idea of a penalty 
function technique is to transform the constrained optimization problem 
into a sequence of unconstrained problems, for which a variety of 
methods are very effective.  The constraints are incorporated into 
the objective in appropriate functions and then they are effectively 
deleted from the problem.  The augmented objective is optimized in 
a sequence of steps in which the contributions of the penalty functions 
are progressively diminished.  In the limit, their contributions become 
negligible and the optimum of the augmented objective is also that 
of the original one.</P>

<P>The modified objective constructed by JOVE is 

&nbsp;&nbsp;&nbsp;&nbsp;<i>P</i> = <i>f</i> - &rho;&Sigma; ln <i>g</i> + &Sigma; h<SUP>2</SUP>/&rho;<BR>

where<i> f</i>, <i>g</i>, and <i>h</i> are the original objective, 
the inequality constraints, and the equality constraints, respectively.  The 
parameter &rho; is a weighting factor which is  decreased for each 
unconstrained suboptimization until <i>P</i> &rarr; <i>f</i> 
and the problem is solved.  The method employed in each of the unconstrained 
optimization steps is to select a direction by a generalized Newton 
procedure and then to locate an optimum in that direction by a modified 
Fibonacci search.  Upon option, when the problem is believed to be 
ill-conditioned, the gradient is used in selecting a direction whenever 
the Hessian matrix of the augmented objective is indefinite (where 
it has eigenvalues of mixed signs).</P>

<P>JOVE provides acceleration controls which, for well-behaved functions, 
can significantly speed convergence.  Essentially, the technique may 
be adjusted to begin extrapolating extrema rather than actually calculating 
them once sufficient steps have been taken.</P>
<hr>

<A NAME="JOVE_CTL" ID="JOVE_CTL"></a>
<table border=2>
<caption>The control variables for <b>JOVE</b> are as follows. Note: In these descriptions,<br>
the term "iteration" refers to a complete unconstrained optimization.
</caption>

<tr bgcolor=#00FFFF>
<th align=left>Variable</th>
<th align=left>Preset<br>Value</th>
<th align=left>Value</th>
<th align=left>Option</th>
</tr>

<TR>
<TD rowspan=2><B>DETAIL</B></TD>
<TD rowspan=2>1</TD>
<TD>0</TD>
<TD>No detail iteration point </TD>
</TR>

<TR>
<TD>n</TD>
<TD>Detailed print every nth iteration plus first and last</TD>
</TR>

<TR>
<TD rowspan=2><B>SUMMARY</B></TD>
<TD rowspan=2>1</TD>
<TD>1</TD>
<TD>Print iteration summary</TD>
</TR>

<TR>
<TD>0</TD>
<TD>No iteration summary</TD>
</TR>

<TR>
<TD><B>REMAX</B></TD>
<TD>5</TD>
<TD></TD>
<TD>Maximum number of iterations</TD>
</TR>

<TR>
<TD><B>STEPSLIM</B></TD>
<TD></TD>
<TD></TD>
<TD>Limit on the number of steps per 100 iteration.  Since an iteration amounts to an entire 
unconstrained optimization, this control is analogous to REMAX for solver HERA. 
However, if solving a constrained problem, exceeding LIMSTEPS does not  terminate 
the process, it merely halts the current iteration.</TD>
</TR>

<TR>
<TD rowspan=3><B>STEPOUT</B></TD>
<TD rowspan=3>5</TD>
<TD>n</TD>
<TD>Provides snapshot prints of the step-by-step 
solution progress at every nth step of all iterations
(about 4-9 lines of critical problem data).</TD>
</TR>

<TR>
<TD>-n</TD>
<TD>Constraint values are added to the snapshot prints.</TD>
</TR>			

<TR>
<TD>0</TD>
<TD>No snapshot prints are generated.</TD>
</TR>

<TR>
<TD><B>EVALMAX</B></TD>
<TD>1000</TD>
<TD></TD>
<TD>Limit on total number of model evaluations 
The optimization process is halted when this limit
is exceeded and the result is flagged in the same
manner as  if maximum iterations were reached.
The actual number of evaluations may exceed
EVALMAX slightly because of the need to terminate
the solution process correctly.</TD>
</TR>
			
<TR>
<TD rowspan=2><B>ABORT</B></TD>
<TD rowspan=2>1</TD>
<TD>0</TD>
<TD>Delay a model abort until the current 
iteration is complete. (This control is only
effective when a model abort is requested by
an abort statement.</TD>
</TR>

<TR>
<TD>1</TD>
<TD>Execute a model abort after the current step in which an ABORT statement was executed.</TD>
</TR>

<TR>
<TD rowspan=2><B>SEARCH</B></TD>
<TD rowspan=2>1</TD>
<TD>1</TD>
<TD>Use generalized Newton-Raphson method</TD>
</TR>

<TR>
<TD>2</TD>
<TD>As above, but use the gradient when the Hessian is indefinite.</TD>
</TR>

<TR>
<TD rowspan=4><B>SETRHO</B></TD>
<TD rowspan=4>1</TD>
<TD>1</TD>
<TD>Set the initial value of &rho; to 1</TD>
</TR>

<TR>
<TD>2</TD>
<TD>Compute an initial value of <i> &rho;</i> which minimizes
the magnitude of the  gradient at the initial point.</TD>
</TR>

<TR>
<TD>3</TD>
<TD>Compute an initial value of <i> &rho;</i> which minimizes 
(<i>&part;P/&part;x)(&Delta;x)</i> at the initial point.</TD>
</TR>

<TR>
<TD>n</TD>
<TD>(0&lt;&lt;n&lt;&lt;1) set value of <i> &rho;</i> to n (necessary for 
restarting a problem from a previously computed point - the appropriate value for <i> &rho;</i>
is printed in the detailed report.)</TD>
</TR>                           	

<TR>
<TD><B>RATIO</B></TD>
<TD>16</TD>
<TD></TD>
<TD>Factor used to reduce &rho; between successive 
unconstrained optimizations: must be &gt; 2</TD>
</TR>

<TR>
<TD rowspan=3><B>SPEEDUP</B></TD>
<TD rowspan=3>1</TD>
<TD>1</TD>
<TD>Do not accelerate by performing extrapolation.</TD>
</TR>

<TR>
<TD>2</TD>
<TD>Perform first order extrapolation.</TD>
</TR>

<TR>
<TD>3</TD>
<TD>Perform second order extrapolation.</TD>
</TR>

<TR>
<TD rowspan=3><B>ESTIMATE</B></TD>
<TD rowspan=3>1</TD>
<TD>1</TD>
<TD>Determine final problem convergence on the basis of 
the current (zero order) solution.</TD>
</TR>

<TR>
<TD>2</TD>
<TD>Base convergence upon either the zero order
solution or on a first order extrapolation.</TD>
</TR>

<TR>
<TD>3</TD>
<TD>Base convergence upon either the zero order
solution or on a second order extrapolation.</TD>
</TR>

<TR>
<TD rowspan=3><B>CONVERGE</B></TD>
<TD rowspan=3>1</TD>
<TD>1</TD>
<TD>Assume that the problem finally converged when 
the scaled objective functions of the primal and 
dual solutions differ by less than ACCURACY.</TD>
</TR>

<TR>
<TD>2</TD>
<TD>Converge when the penalty contribution of the inequality constraints is less than ACCURACY.</TD>
</TR>			

<TR>
<TD>3</TD>
<TD>Converge when the scaled first order estimate
of the optimum objective differs from the dual
objective by less than  ACCURACY.</TD>
</TR>

<TR>
<TD><B>ACCURACY</B></TD>
<TD>10<SUP>-4</SUP></TD>
<TD></TD>
<TD>Final convergence tolerance</TD>
</TR>

<TR>
<TD><B>PROGRESS</B></TD>
<TD>0</TD>
<TD></TD>
<TD>Iteration improvement limit; an unconstrained optimization 
is terminated if the objective fails to improve by at least this amount.</TD>
</TR>
			
<TR>
<TD rowspan=3><B>SUBTEST</B></TD>
<TD rowspan=3>1</TD>
<TD>1</TD>
<TD>Terminate an iteration (an unconstrained optimization) 
when <i>(&part;P/&part;x<SUB>i</SUB>)</i> &lt; ZERO 
for all <i> x<SUB>i</SUB></i>.</TD>
</TR>

<TR>
<TD>2</TD>
<TD>Terminate an iteration when (<i>&part;P/&part;x<SUB>i</SUB></i>)
(&Delta;x<SUB>i</SUB>) is less than 20% of the improvement in the 
augmented objective for all <i>x<SUB>i</SUB></i></TD>
</TR>

<TR>
<TD>3</TD>
<TD>Terminate an iteration when |&part;P<D>/&part;x<SUB>i</SUB>| &lt; ZERO
for all <i>x<SUB>i</SUB>.</i></TD>
</TR>

<TR>
<TD><B>ZERO</B></TD>
<TD>10<SUP>-6</SUP></TD>
<TD></TD>
<TD>Iteration convergence tolerance</TD>
</TR>

<TR>
<TD rowspan=2><B>BREAKIN</B></TD>
<TD rowspan=2>0</TD>
<TD>0</TD>
<TD>No interactive breakpoints</TD>
</TR>

<TR>
<TD>n</TD>
<TD>Breakpoint after each n'th iteration</TD>
</TR>

<TR>
<TD rowspan=3><B>DETOUT</B></TD>
<TD rowspan=3>+1</TD>
<TD>0</TD>
<TD>Detailed report to PRINTER </TD>
</TR>

<TR>
<TD>+1</TD>
<TD>Detailed report to SCROLL</TD>
</TR>

<TR>
<TD>-1</TD>
<TD>Detailed report to Console</TD>
</TR>

<TR>
<TD rowspan=3><B>SUMOUT</B></TD>
<TD rowspan=3>+1</TD>
<TD>0</TD>
<TD>Summary report to PRINTER</TD>
</TR>

<TR>
<TD>+1</TD>
<TD>Summary report to SCROLL<BR></TD>
</TR>

<TR>
<TD>-1</TD>
<TD>Summary report to Console</TD>
</TR>

</TABLE>			
<HR>
<P>This rather disconcertingly long list of controls basically provides 
capabilities for tuning JOVE for difficult problems.  For many problems, 
and for virtually all simple ones, the preset values should be effective.  When 
the need arises to reset controls, the following considerations should 
be helpful.</P>

<P>When the circumstances permit, computing an estimate for &rho; is 
desirable, rather than picking the arbitrary value of 1.  However, 
this is only possible if there are no equality constraints. Furthermore, 
the best approximation (SETRHO=3) is further limited by the requirement 
that the initial point must be close to some inequality constraint 
boundary (if any exist).</P>

<P>The preset RATIO works well for convex problems.  A slower reduction 
of &rho; may be required for nonconvex problems. </P> 

<P>Either of CONVERGE = 1 or 3 are theoretically consistent with the 
solution technique.  The second option ignores the contribution of 
equality constraints, which may be appropriate whenever such constraints 
are weakly imposed.  Only the preset test is allowed whenever convergence 
estimation is activated (ESTIMATE &ne; 1).</P>

<P>The iteration improvement limit is inactive in the preset condition. 
When JOVE is being used to obtain a rough estimate at which to begin 
a refined optimization, the value of PROGRESS might be increased.</P>

<P>The value of ACCURACY essentially bounds the accuracy of the final 
solution: 10<SUP>-4</SUP>  assures four figure accuracy.  If the unconstrained 
subproblems are being solved to a high precision (ZERO=10<SUP>-6</SUP>), 
the final solution will normally have an accuracy substantially better 
than the upper bound.</P>

<P><B><i>Optimization Summary Report</i></B>  -  A standard optimization summary 
report is nominally generated by JOVE.  In format and content, it 
is identical to that produced by HERA (see Figure 6-2).   It must 
be remembered that an iteration, for JOVE, is a complete unconstrained 
optimization.  Thus, when this solver is used for unconstrained optimization, 
it will converge in a single iteration.</P>

<H3> Figure 7-1  JOVE Optimization Summary Report</H3>
<PRE>
 --- JOVE SUMMARY, INVOKED AT SIMPLEJ[5] FOR MODEL MODF ----

    CONVERGENCE CONDITION AFTER 10 ITERATIONS
       OBJECTIVE CRITERION SATISFIED
      ALL SPECIFIED CRITERIA SATISFIED

  LOOP NUMBER .........   [INITIAL]         1             2
  UNKNOWNS
    X                  -0.100000E+01  0.115943E+01  0.119551E+01
    Y                   0.000000E+00  0.658668E+00  0.800625E+00
  OBJECTIVE
    F                   0.100000E+02  0.823059E+00  0.686951E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.230085E+00  0.168779E-02
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.101327E+01 -0.591231E+00

  LOOP NUMBER .........   [INITIAL]         3             4
  UNKNOWNS
    X                  -0.100000E+01  0.117343E+01  0.117181E+01
    Y                   0.000000E+00  0.809790E+00  0.810379E+00
  OBJECTIVE
    F                   0.100000E+02  0.719403E+00  0.721853E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.692142E-05  0.270073E-07
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587123E+00 -0.587106E+00

  LOOP NUMBER .........   [INITIAL]         5             6
  UNKNOWNS
    X                  -0.100000E+01  0.117171E+01  0.117170E+01
    Y                   0.000000E+00  0.810416E+00  0.810419E+00
  OBJECTIVE
    F                   0.100000E+02  0.722009E+00  0.722018E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.106743E-09  0.563216E-12
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587106E+00 -0.587106E+00

  LOOP NUMBER .........   [INITIAL]         7             8
  UNKNOWNS
    X                  -0.100000E+01  0.117170E+01  0.117170E+01
    Y                   0.000000E+00  0.810419E+00  0.810419E+00
  OBJECTIVE
    F                   0.100000E+02  0.722018E+00  0.722018E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.364753E-11  0.849432E-12
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587106E+00 -0.587106E+00

  LOOP NUMBER .........   [INITIAL]         9            10
  UNKNOWNS
    X                  -0.100000E+01  0.117170E+01 -0.655608E+00
    Y                   0.000000E+00  0.810419E+00 -0.451401E+00
  OBJECTIVE
    F                   0.100000E+02  0.722018E+00  0.915882E+01
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.302314E-12  0.688782E+00
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587106E+00 -0.226111E-08

 ---END OF LOOP SUMMARY
</PRE>

<P>If the initial estimate of the unknowns is not within the feasible 
region, JOVE searches for a feasible point as a first step.  This 
circumstance is flagged in the first iteration print, if one is requested.  In 
any case, the summary of the first iteration presents the results 
for the first optimization which began at a feasible point.</P>

<P><B><i>Detailed Iteration Report </i></B> -  A detailed iteration report 
is issued by JOVE whenever the control DETAIL is nonzero.  This report 
actually summarizes an unconstrained suboptimization.  This report 
lists the current value of &rho; and the evaluated penalty function 
gradient vector &nabla;P.  The solution is presented for the 
original objective, the unknowns, the constraints, and the augmented 
objective function (the primal penalized objective).  The contributions 
of each class of constraints to the penalty function is also displayed.</P>

<P>The optimization technique signifies which SEARCH has been chosen 
for unconstrained optimization.</P>

<P>A sample iteration print generated while solving the SIMPLE problem 
described earlier is shown in Figure 7-2.</P>

<H3> Figure 7-2  JOVE Detailed Iteration Report </H3>
<PRE>
 ---- JOVE ITERATION 3 INVOKED AT SIMPLEJ[5] FOR MODEL MODF ----

   OPTIMIZATION TECHNIQUE ... GENERALIZED NEWTON-RAPHSON

   SUB-OPTIMIZATION INITIAL CONDITIONS AT RHO = 0.390625E-02

      PENALTY FUNCTION GRADIENT VECTOR
       0.156729E+01   0.431828E+01

      GRADIENT VECTOR NORM = 0.459390E+01

   TERMINAL CONDITIONS AFTER 25 STEPS

     OBJECTIVE FUNCTION = 0.719403E+00

     PENALIZED OBJECTIVES ... PRIMAL = 0.890125E+02 DUAL = 0.177209E+03

       CUMULATIVE MODEL EVALUATIONS = 844

     INDEPENDENT VARIABLES
      0.117343E+01   0.809790E+00

     INEQUALITY CONSTRAINTS, WITH PENALTY CONTRIBUTION  0.464097E-01
      0.692142E-05

     EQUALITY CONSTRAINTS, WITH PENALTY CONTRIBUTION  0.882466E+02
     -0.587123E+00

    CONVERGENCE CONDITION AFTER  3 ITERATIONS
       OBJECTIVE CRITERION UNSATISFIED
       SPECIFIED CRITERIA UNSATISFIED

 ---END JOVE ITERATION 3
</PRE>

</body>
</html>
