<H1>7. Constrained Optimization </H1>
<P>
In the vast majority of practical applications of optimization, there 
are limitations on the acceptable solutions.  Not all values of the 
unknowns are allowable or even feasible.  In a bridge design, beam 
members cannot exceed certain bearing loads.  In budget planning, 
depreciation rates must follow fixed schedules.  There are practical 
limitations on sources of supply, container size, temperature gradients, 
etc.</P>

<P>Limitations of this type are expressed as constraints on the optimization 
variables, and a typical constrained optimization problem may be expressed 
mathematically as </P>

&nbsp;&nbsp;&nbsp;&nbsp;<i>find x<SUB>1</SUB>, ..., x<SUB>n</SUB>&nbsp;&nbsp; in the range l<SUB>i</SUB> 
&le; x<SUB>i</SUB> &le; u<SUB>i</SUB>&nbsp;&nbsp;&nbsp;&nbsp; for i=1,...,n</i><BR>

&nbsp;&nbsp;&nbsp;&nbsp;<i>which minimizes f = f(x<SUB>1</SUB>, ..., x<SUB>n</SUB>)</i><BR>

&nbsp;&nbsp;&nbsp;&nbsp;<i>subject to the constraint functions</i>

&nbsp;&nbsp;&nbsp;&nbsp;<i>g<SUB>j</SUB>(x<SUB>1</SUB>, ..., x<SUB>n</SUB>) &ge; 
0        (j=1, ..., l)</i>

&nbsp;&nbsp;&nbsp;&nbsp;<i>h<SUB>k</SUB>(x<SUB>1</SUB>, ..., x<SUB>n</SUB>) = 0      (k=1, 
..., m)</i>

<P>In contrast with unconstrained optimization problems, the objective 
function <i>f</i> is represented by a set of formulas which depend 
upon some, though not necessarily all, of the unknowns <i>x<SUB>1</SUB>, 
..., x<SUB>n</SUB></i>.  Those unknowns which do not appear in the objective 
must enter into the constraints in some way or else they are not really 
unknowns but rather simply auxiliary design variables.</P>

<P>The constant constraints<i> l<SUB>i</SUB> </i>and <i>u<SUB>i</SUB> </i>together 
with the inequality functions <i>g<SUB>j</SUB> </i>prescribe a feasible 
region in which solutions are acceptable and are phrased, in a standard 
fashion, as positive constraints.  Essentially, these constraints 
are imposed to assure that any solution which is reached is physically 
reasonable and compatible with the problem being analyzed.</P>

<P>The equality constraints <i>h<SUB>k</SUB></i> serve a different purpose.  They 
express relationships between the unknowns which cannot be simply 
solved by explicit equations.  Constraints of this type might include 
physical laws, such as conservation of energy, or empirical relationships.  Properly 
defined equality constraints, therefore, are expressed as implicit 
functions; and, as noted in the previous section, they could be treated 
separately as a part of a combined calculus process.</P>

<P>In FC, the above optimization problem is expressed by a statement 
of the form:<BR><BR>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>x<SUB>1</SUB>, ..., x<SUB>n</SUB></i><B>; IN</B> <i>model</i><B>; 
BY</B> <i>solver</i>;<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>WITH LOWER</B> <i>l<SUB>1</SUB>, ..., l<SUB>n</SUB></i><B>; AND UPPER</B> 
<i>u<SUB>1</SUB>, ..., u<SUB>n</SUB></i><B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>HOLDING</B> <i>g<SUB>1</SUB>, ..., g<SUB>l</SUB></i><B>; MATCHING</B>  <i>h<SUB>1</SUB>, 
..., h<SUB>m</SUB></i>;<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B> <i>f</i><BR><BR>

where <i>x<SUB>1</SUB>, ..., x<SUB>n</SUB></i> are the names of the unknowns 
whose values are to be determined and <i>f</i> is an objective variable 
to be minimized. The <i>model</i> identifies a MODEL procedure whose 
execution computes both <i>f</i> and all of the constraints, <i>g</i> 
and <i>h</i>, for a given set of values of <i>x<SUB>1</SUB>, ..., x<SUB>n</SUB></i>.  In 
general, there may be any number of constraints and they may take 
any functional form.</P>

<P>The solution process, like that of unconstrained optimization, involves 
successive approximation of the unknowns <i>x<SUB>1</SUB>, ..., x<SUB>n</SUB></i> 
by iterative execution of the <i>model</i> until a local optimum value 
of <i>f</i> is found.  In this case, however, the solution is held 
within the feasible region by one of a variety of techniques.  As 
usual, an initial estimate must be made for the values of the unknowns 
before the calculus process is invoked.  If this estimate is infeasible, 
the solution process first attempts to find a feasible starting point, 
typically employing the same general technique as it will use thereafter.</P>

<P><B><i>Specifying Constraints</i></B>  -  Constraints have two forms in FC. 
Constant constraints such as<br><BR>

&nbsp;&nbsp;&nbsp;&nbsp;x &ge; 0<BR>                                                                                                            

&nbsp;&nbsp;&nbsp;&nbsp;y &ge; 0<BR>                                                                                                            

&nbsp;&nbsp;&nbsp;&nbsp;a &ge; x &ge; b<br><BR>                                                                                      

are specified at the beginning of the optimization process, and may 
not be changed while it is in progress.</P>

<P>Constraint functions such as<br><BR>

&nbsp;&nbsp;&nbsp;&nbsp;e<SUP>x</SUP> +e<SUP>y</SUP> = 1<BR>                                                                                                           

&nbsp;&nbsp;&nbsp;&nbsp;xy &le; 100<br><BR>                                                                                                         

are dependent on the values of the unknowns. Thus they are calculated 
in the model.  Where there are explicit equality relationships between 
model variables, they do not properly represent constraints.  Thus, 
in a problem with two variables x and y where<br><BR>

&nbsp;&nbsp;&nbsp;&nbsp;x + y = 1<br><BR>

only one may be correctly viewed as an unknown.  Of course, there 
is no reason to eliminate one of the variables from the model, particularly 
since it might represent a design parameter of interest. Rather, it 
should not be specified as an unknowns, and the equation should be 
treated as an auxiliary calculation rather than a constraint.</P>

<P><i>Functions</i>  -  In generating the model functions which represent 
constraints, standard conditions must be met.  All equality constraints 
are matched to zero, and thus the fourth constraint above must be 
calculated by <br> <BR>

&nbsp;&nbsp;&nbsp;&nbsp;h = 1 - e<SUP>x</SUP> - e<SUP>y</SUP> = 0<br><BR>

Similarly, all inequality constraints are expressed as positive functions, 
whence the fifth  constraint is expressed as <br> <BR>

&nbsp;&nbsp;&nbsp;&nbsp;g = 100 - xy &ge; 0 <BR>

<P>For the sake of simplicity, all of these examples involve few variables 
and can be expressed concisely.  There are, however, no limits on 
the complexity of the relationships; they may require whole series 
of calculations, even involving calculus processes.  The only requirement 
is that the values finally computed meet the standard mathematical 
conditions.  Indeed, the constraints need not be computed directly 
from the unknowns at all, but may be expressed as functions of any 
variables which are dependent upon one or more unknowns.</P>

<P>Care must be exercised to assure that constraints describe a feasible 
problem, particularly when they are complex and when there are many 
auxiliary variables.   Furthermore, all constraints must depend, directly 
or indirectly, on at least one unknown.</P>

<P><i>Constants</i>  -  Constant constraints may obviously be expressed 
as "trivial" functions.  However, it is far better to express them 
as limits prior to the start of optimization, since all solvers work 
more efficiently and exactly in this manner.  Either upper or lower 
limits (or both) may be specified.  However, when either type of limit 
is specified a value must be specified for every unknown.</P>

<P>If the upper and lower limits on any unknown overlap, they are ignored. 
Thus it is possible to limit some variables and not others by setting 
the upper and lower limits the same (or overlapping) for those variables 
which are to remain unlimited.</P>

<P><B><i>Algorithm levels</B></i>-  Due to the influence of first-order 
unconstrained optimization methods and the broad use of penalty-function 
stratagies, most constrained optimization algorithms are hierarchic 
combinations of three algorithmic levels. The highest level is usually 
referred to as the <i>strategy level</i>.  This is the level at which 
problems are specified using the FIND statement.  Each strategy iteration 
is a complete solution of a transformed problem, usually an unconstrained 
optimization problem.  Between strategy iterations, the transformed 
problem is altered according to the strategy algorithm so that the 
solution to each successive transformed problem is a closer approximation 
to the constrained optimization problem posed in the FIND statement. 
The second iterative level generally corresponds to unconstrained 
optimization (although in some cases constraints are directly addressed 
at this level as well).  It is therefore referred to as the <i>optimizer 
level</i>.  Each optimizer iteration selects a search vector in the 
coordinate system of the unknowns.  Thus the model calls at this level  
generate gradient vectors or Hessian matrices for use in computing 
the direction vector.  Since many optimizers perform a one-dimensional 
search along the direction vector, seeking either a local extremum 
or a constraint boundary, the third iterative level is called the 
<i>vector or one-dimensional search</i> <i>level</i>.  In most cases, 
this does not involve differentiation, thus model calls at this level 
involve only ordinary arithmetic.
<H2>7.1. Solver JOVE</H2>

<P>The solver JOVE is a sequential unconstrained optimization technique. The 
theoretical basis for this technique is thoroughly treated in <i>Nonlinear 
Programming: Sequential Unconstrained Minimization Techniques</i>, 
Fiacco, A.V. and McCormick, G.P. (Wiley 1968).  In this method, 
a new objective function is constructed from the original one by adding 
penalty functions involving the constraints. Then this augmented objective 
is optimized by an unconstrained optimization technique.</P>

<P>The general form of the FIND statement for JOVE is:<BR><BR>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>model</i><B>; BY JOVE</B> 
{<B>(</B><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</b>}  {<B>WITH</B>|<B>AND</B>} 
<B>UPPER</b> <i>ceiling</i><B>;</B>}  <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {<B>WITH FLAG</B> <i>signal</i><b>;</b>}   <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</B>}  {<B>MATCHING</B> <i>equalities</i><B>;</B>} 
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i> <BR><BR>                                                       

where the optional clauses may appear in any order.  The common elements, 
<i>unknowns</i>, <i>model</i>, <i>controller</i>, and <i>auxiliaries</i>, 
are exactly as described for the general FIND statement in Section 
2.2.2.  The elements <i>floor</i> and <i>ceiling</i> are lists containing 
constant constraints. If present, these lists must correspond in size 
to the <i>unknowns</i> list. The FLAG parameter <i>signal</i> permits 
JOVE to signal the condition of problem solution, as explained  in 
Section 2.2.5.  The constraints if any, are specified in the HOLDING 
and MATCHING clauses.  Each prescribes a list of variables whose values 
represent the constraints.  The final objective clause identifies 
the function to be optimized and the desired type of extremum.</P>

<P>When JOVE is invoked by execution of the FIND statement, it activates 
derivative evaluation to compute gradients and Hessians with respect 
to the <i>unknowns</i>.</P>

<H3>Example</h3>

&nbsp;&nbsp;&nbsp;&nbsp;Find x,y which minimizes f = (x-2)<SUP>2</SUP> + (y-1)<SUP>2</SUP><BR>

&nbsp;&nbsp;&nbsp;&nbsp;Subject to:<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; g(x,y) = 1 -x<SUP>2</SUP>/4 - y<M^>2 &ge; 0<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h(x,y) = e<SUP>xy</SUP> - x - 2  = 0

&nbsp;&nbsp;&nbsp;&nbsp;starting from the feasible point (-1,0)

<P>Program:</P>
<PRE>
      PROBLEM SIMPLE 
        COMMON/EQS/X,Y,G,H,F 
        X=-1 : Y=0  ! Initial estimates 
        FIND X,Y; IN MODF; BY JOVE; 
          HOLDING G; AND MATCHING H; 
          TO MINIMIZE F 
      END

      MODEL MODF 
        COMMON/EQS/X,Y,G,H,F 
        G=1-X**2/4 - Y**2 
        H=EXP(X*Y) -X -2 
        F = (X-2)**2 + (Y-1)**2 
      END    
</PRE>


<P><B><i>The Solution Process </i></B> -  The essential idea of a penalty 
function technique is to transform the constrained optimization problem 
into a sequence of unconstrained problems, for which a variety of 
methods are very effective.  The constraints are incorporated into 
the objective in appropriate functions and then they are effectively 
deleted from the problem.  The augmented objective is optimized in 
a sequence of steps in which the contributions of the penalty functions 
are progressively diminished.  In the limit, their contributions become 
negligible and the optimum of the augmented objective is also that 
of the original one.</P>

<P>The modified objective constructed by JOVE is 

&nbsp;&nbsp;&nbsp;&nbsp;<i>P</i> = <i>f</i> - &rho;&Sigma; ln <i>g</i> + &Sigma; h<SUP>2</SUP>/&rho;<BR>

where<i> f</i>, <i>g</i>, and <i>h</i> are the original objective, 
the inequality constraints, and the equality constraints, respectively.  The 
parameter &rho; is a weighting factor which is  decreased for each 
unconstrained suboptimization until <i>P</i> &rarr; <i>f</i> 
and the problem is solved.  The method employed in each of the unconstrained 
optimization steps is to select a direction by a generalized Newton 
procedure and then to locate an optimum in that direction by a modified 
Fibonacci search.  Upon option, when the problem is believed to be 
ill-conditioned, the gradient is used in selecting a direction whenever 
the Hessian matrix of the augmented objective is indefinite (where 
it has eigenvalues of mixed signs).</P>

<P>JOVE provides acceleration controls which, for well-behaved functions, 
can significantly speed convergence.  Essentially, the technique may 
be adjusted to begin extrapolating extrema rather than actually calculating 
them once sufficient steps have been taken.</P>
<A NAME="JOVE_CTL" ID="JOVE_CTL"><B><i>JOVE Controls</i></B></A>- The control variable for JOVE are as follows. 
[FOOTNOTE2] Note: In 
these descriptions, the term "iteration" refers to a complete unconstrained 
optimization.
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH>
	<TH align=center style="width: 599px">Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>
<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD style="width: 599px">No detail iteration point </TD><TD>1</TD></TR>
<TR><TD></TD><TD>n</TD><TD style="width: 599px">Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD style="width: 599px">Print iteration summary </TD><TD>1</TD></TR>
<TR><TD></TD><TD>0</TD><TD style="width: 599px">No iteration summary</TD><TD></TD></TR>

<TR><TD><B>REMAX</B></TD><TD></TD><TD style="width: 599px">Maximum number of iterations</TD><TD>5</TD></TR>

<TR><TD><B>STEPSLIM</B></TD><TD></TD><TD style="width: 599px">Limit on the number of steps per 100 iteration. 
                                         Since an iteration amounts to an entire 
                                         unconstrained optimization, this control 
                                         is analogous to REMAX for solver HERA. 
                                         However, if solving a constrained problem, 
                                         exceeding LIMSTEPS does not  terminate 
			                             the process, it merely halts the current iteration.</TD><TD>
	100</TD></TR>

<TR><TD><B>STEPOUT</B></TD><TD>n</TD><TD style="width: 599px">Provides snapshot prints of the step-by-step 
                                         solution progress at every nth step of all iterations
			                             (about 4-9 lines of critical problem data).</TD><TD>5</TD></TR>
<TR><TD></TD><TD>-n</TD><TD style="width: 599px">Constraint values are added to the snapshot prints.</TD><TD></TD></TR>			
<TR><TD></TD><TD>0</TD><TD style="width: 599px">No snapshot prints are generated.</TD><TD></TD></TR>

<TR><TD><B>EVALMAX</B></TD><TD></TD><TD style="width: 599px">Limit on total number of model evaluations 
                                        The optimization process is halted when this limit
	                                    is exceeded and the result is flagged in the same
			                            manner as  if maximum iterations were reached.
			                            The actual number of evaluations may exceed
			                            EVALMAX slightly because of the need to terminate
			                            the solution process correctly.</TD><TD>1000</TD></TR>
			
<TR><TD><B>ABORT</B></TD><TD>0</TD><TD style="width: 599px">Delay a model abort until the current 
                                       iteration is complete. (This control is only
                                       effective when a model abort is requested by
                                       an abort statement.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>1</TD><TD style="width: 599px">Execute a model abort after the 
                           current step in which an ABORT 
			               statement was executed.</TD><TD></TD></TR>


<TR><TD><B>SEARCH</B></TD><TD>1</TD><TD style="width: 599px">Use generalized Newton-Raphson method</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD style="width: 599px">As above, but use the gradient when the 
			               Hessian is indefinite.</TD><TD></TD></TR>
			

<TR><TD><B>SETRHO</B></TD><TD>1</TD><TD style="width: 599px">Set the initial value of &rho; to 1</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD style="width: 599px">Compute an initial value of <i> &rho;</i> which minimizes
			               the magnitude of the  gradient at the initial point.</TD><TD></TD></TR>
<TR><TD></TD><TD>3</TD><TD style="width: 599px">Compute an initial value of <i> &rho;</i> which minimizes 
			(<i>&part;P/&part;x)(&Delta;x)</i> at the initial point.</TD><TD></TD></TR>
<TR><TD></TD><TD>n</TD><TD style="width: 599px">(0&lt;&lt;n&lt;&lt;1) set value of <i> &rho;</i> to n (necessary for 
			               restarting a problem from a previously 
			               computed point - the appropriate value for <i> &rho;</i>
			               is printed in the detailed report.)</TD><TD></TD></TR>                           	
<TR><TD><B>RATIO</B></TD><TD></TD><TD style="width: 599px">Factor used to reduce &rho; between successive 
                                      unconstrained optimizations: must be &gt; 2</TD><TD>16<BR></TD></TR>
			

<TR><TD><B>SPEEDUP</B></TD><TD>1</TD><TD style="width: 599px">Do not accelerate by performing extrapolation.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD style="width: 599px">Perform first order extrapolation.</TD><TD></TD></TR>
<TR><TD></TD><TD>3</TD><TD style="width: 599px">Perform second order extrapolation.</TD><TD></TD></TR>

<TR><TD><B>ESTIMATE</B></TD><TD>1</TD><TD style="width: 599px">Determine final problem convergence on the basis of 
                                          the current (zero order) solution.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD style="width: 599px">Base convergence upon either the zero order
			               solution or on a first order extrapolation.</TD><TD></TD></TR>
<TR><TD></TD><TD>3</TD><TD style="width: 599px">Base convergence upon either the zero order
			               solution or on a second order extrapolation.</TD><TD></TD></TR>

<TR><TD><B>CONVERGE</B></TD><TD>1</TD><TD style="width: 599px">Assume that the problem finally converged when 
                                          the scaled objective functions of the primal and 
			                              dual solutions differ by less than ACCURACY.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD style="width: 599px">Converge when the penalty contribution of the
			               inequality constraints is less  than ACCURACY.</TD><TD></TD></TR>			
<TR><TD></TD><TD>3</TD><TD style="width: 599px">Converge when the scaled first order estimate<BR>
			               of the optimum objective differs from the dual<BR>
			               objective by less than  ACCURACY.</TD><TD></TD></TR>

<TR><TD><B>ACCURACY</B></TD><TD></TD><TD style="width: 599px">Final convergence tolerance</TD><TD>10<SUP>-4</SUP></TD></TR>

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD style="width: 599px">Iteration improvement limit; an unconstrained optimization 
                                        is terminated if the objective fails to improve by at least 
                                        this amount.</TD><TD>0</TD></TR>
			
<TR><TD><B>SUBTEST</B></TD><TD>1</TD><TD style="width: 599px">Terminate an iteration (an unconstrained optimization) 
                                         when <i>(&part;P/&part;x<SUB>i</SUB>)</i> &lt; ZERO 
                                         for all <i> x<SUB>i</SUB></i>.</TD><TD>1<BR></TD></TR>
<TR><TD></TD><TD>2</TD><TD style="width: 599px">Terminate an iteration when (<i>&part;P/&part;x<SUB>i</SUB></i>)
                           (&Delta;x<SUB>i</SUB>) is less than 20% of the improvement in the 
                           augmented objective for all <i>x<SUB>i</SUB><BR></TD><TD></TD></TR>			
<TR><TD></TD><TD>3</TD><TD style="width: 599px">Terminate an iteration when |&part;P<D>/&part;x<SUB>i</SUB>| &lt; ZERO
			               for all <i>x<SUB>i</SUB>.</TD><TD></TD></TR>
			
<TR><TD><B>ZERO</B></TD><TD></TD><TD style="width: 599px">Iteration convergence tolerance </TD><TD>10<SUP>-6</SUP></TD></TR>

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD style="width: 599px">No interactive breakpoints </TD><TD>0<BR></TD></TR>
<TR><TD></TD><TD>n</TD><TD style="width: 599px">Breakpoint after each nth iteration</TD><TD></TD></TR>
			
<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD style="width: 599px">Detailed report to PRINTER </TD><TD>+1<BR></TD></TR>

<TR><TD></TD><TD>+1</TD><TD style="width: 599px">Detailed report to SCROLL </TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD style="width: 599px">Detailed report to Console</TD><TD></TD></TR>
			
<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD style="width: 599px">Summary report to PRINTER</TD><TD>+1<BR></TD></TR>

<TR><TD></TD><TD>+1</TD><TD style="width: 599px">Summary report to SCROLL<BR></TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD style="width: 599px">Summary report to Console</TD><TD></TD></TR>
</TABLE>			
<HR>
<P>This rather disconcertingly long list of controls basically provides 
capabilities for tuning JOVE for difficult problems.  For many problems, 
and for virtually all simple ones, the preset values should be effective.  When 
the need arises to reset controls, the following considerations should 
be helpful.</P>

<P>When the circumstances permit, computing an estimate for &rho; is 
desirable, rather than picking the arbitrary value of 1.  However, 
this is only possible if there are no equality constraints. Furthermore, 
the best approximation (SETRHO=3) is further limited by the requirement 
that the initial point must be close to some inequality constraint 
boundary (if any exist).</P>

<P>The preset RATIO works well for convex problems.  A slower reduction 
of &rho; may be required for nonconvex problems. </P> 

<P>Either of CONVERGE = 1 or 3 are theoretically consistent with the 
solution technique.  The second option ignores the contribution of 
equality constraints, which may be appropriate whenever such constraints 
are weakly imposed.  Only the preset test is allowed whenever convergence 
estimation is activated (ESTIMATE &ne; 1).</P>

<P>The iteration improvement limit is inactive in the preset condition. 
When JOVE is being used to obtain a rough estimate at which to begin 
a refined optimization, the value of PROGRESS might be increased.</P>

<P>The value of ACCURACY essentially bounds the accuracy of the final 
solution: 10<SUP>-4</SUP>  assures four figure accuracy.  If the unconstrained 
subproblems are being solved to a high precision (ZERO=10<SUP>-6</SUP>), 
the final solution will normally have an accuracy substantially better 
than the upper bound.</P>

<P><B><i>Optimization Summary Report</i></B>  -  A standard optimization summary 
report is nominally generated by JOVE.  In format and content, it 
is identical to that produced by HERA (see Figure 6-2).   It must 
be remembered that an iteration, for JOVE, is a complete unconstrained 
optimization.  Thus, when this solver is used for unconstrained optimization, 
it will converge in a single iteration.</P>

<H3> Figure 7-1  JOVE Optimization Summary Report</H3>
<PRE>
 --- JOVE SUMMARY, INVOKED AT SIMPLEJ[5] FOR MODEL MODF ----

    CONVERGENCE CONDITION AFTER 10 ITERATIONS
       OBJECTIVE CRITERION SATISFIED
      ALL SPECIFIED CRITERIA SATISFIED

  LOOP NUMBER .........   [INITIAL]         1             2
  UNKNOWNS
    X                  -0.100000E+01  0.115943E+01  0.119551E+01
    Y                   0.000000E+00  0.658668E+00  0.800625E+00
  OBJECTIVE
    F                   0.100000E+02  0.823059E+00  0.686951E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.230085E+00  0.168779E-02
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.101327E+01 -0.591231E+00

  LOOP NUMBER .........   [INITIAL]         3             4
  UNKNOWNS
    X                  -0.100000E+01  0.117343E+01  0.117181E+01
    Y                   0.000000E+00  0.809790E+00  0.810379E+00
  OBJECTIVE
    F                   0.100000E+02  0.719403E+00  0.721853E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.692142E-05  0.270073E-07
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587123E+00 -0.587106E+00

  LOOP NUMBER .........   [INITIAL]         5             6
  UNKNOWNS
    X                  -0.100000E+01  0.117171E+01  0.117170E+01
    Y                   0.000000E+00  0.810416E+00  0.810419E+00
  OBJECTIVE
    F                   0.100000E+02  0.722009E+00  0.722018E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.106743E-09  0.563216E-12
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587106E+00 -0.587106E+00

  LOOP NUMBER .........   [INITIAL]         7             8
  UNKNOWNS
    X                  -0.100000E+01  0.117170E+01  0.117170E+01
    Y                   0.000000E+00  0.810419E+00  0.810419E+00
  OBJECTIVE
    F                   0.100000E+02  0.722018E+00  0.722018E+00
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.364753E-11  0.849432E-12
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587106E+00 -0.587106E+00

  LOOP NUMBER .........   [INITIAL]         9            10
  UNKNOWNS
    X                  -0.100000E+01  0.117170E+01 -0.655608E+00
    Y                   0.000000E+00  0.810419E+00 -0.451401E+00
  OBJECTIVE
    F                   0.100000E+02  0.722018E+00  0.915882E+01
  INEQUALITY CONSTRAINTS
    G                   0.750000E+00  0.302314E-12  0.688782E+00
  EQUALITY CONSTRAINTS
    H                   0.000000E+00 -0.587106E+00 -0.226111E-08

 ---END OF LOOP SUMMARY
</PRE>FIG. 7-1. JOVE Optimization Summary Report


<P>If the initial estimate of the unknowns is not within the feasible 
region, JOVE searches for a feasible point as a first step.  This 
circumstance is flagged in the first iteration print, if one is requested.  In 
any case, the summary of the first iteration presents the results 
for the first optimization which began at a feasible point.</P>

<P><B><i>Detailed Iteration Report </i></B> -  A detailed iteration report 
is issued by JOVE whenever the control DETAIL is nonzero.  This report 
actually summarizes an unconstrained suboptimization.  This report 
lists the current value of &rho; and the evaluated penalty function 
gradient vector &nabla;P.  The solution is presented for the 
original objective, the unknowns, the constraints, and the augmented 
objective function (the primal penalized objective).  The contributions 
of each class of constraints to the penalty function is also displayed.</P>

<P>The optimization technique signifies which SEARCH has been chosen 
for unconstrained optimization.</P>

<P>A sample iteration print generated while solving the SIMPLE problem 
described earlier is shown in Figure 7-2.</P>


<PRE>
 ---- JOVE ITERATION 3 INVOKED AT SIMPLEJ[5] FOR MODEL MODF ----

   OPTIMIZATION TECHNIQUE ... GENERALIZED NEWTON-RAPHSON

   SUB-OPTIMIZATION INITIAL CONDITIONS AT RHO = 0.390625E-02

      PENALTY FUNCTION GRADIENT VECTOR
       0.156729E+01   0.431828E+01

      GRADIENT VECTOR NORM = 0.459390E+01

   TERMINAL CONDITIONS AFTER 25 STEPS

     OBJECTIVE FUNCTION = 0.719403E+00

     PENALIZED OBJECTIVES ... PRIMAL = 0.890125E+02 DUAL = 0.177209E+03

       CUMULATIVE MODEL EVALUATIONS = 844

     INDEPENDENT VARIABLES
      0.117343E+01   0.809790E+00

     INEQUALITY CONSTRAINTS, WITH PENALTY CONTRIBUTION  0.464097E-01
      0.692142E-05

     EQUALITY CONSTRAINTS, WITH PENALTY CONTRIBUTION  0.882466E+02
     -0.587123E+00

    CONVERGENCE CONDITION AFTER  3 ITERATIONS
       OBJECTIVE CRITERION UNSATISFIED
       SPECIFIED CRITERIA UNSATISFIED

 ---END JOVE ITERATION 3
</PRE>
<br>Figure 7-2  JOVE Detailed Iteration Report
<H2>7.2. Solver ZEUS </H2>

<P>ZEUS applies the same general approach as JOVE for the solution of 
constrained optimization problems. However, its method for performing 
the unconstrained suboptimizations is a variation of the Davidon-Fletcher-Powell 
(DFP) technique. This is a first-order technique, employing only 
gradients, in contrast with the second-order technique of JOVE. By 
way of comparison, ZEUS will often prove more reliable for problems 
in which the Hessian matrix of the augmented objective is ill-conditioned. </P>

<A NAME="FIND_ZEUS" ID="FIND_ZEUS"><P>The general form of the FIND statement for ZEUS is :<BR><BR></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>model</i><B>; BY 
ZEUS </B>{<B>(</B> <i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>}  {<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}         <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {<B>WITH  FLAG</B> <i>signal</i><B>;</B>}                                 <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>}                               <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR><BR>

Except for the BY solver clause, which invokes ZEUS, the form, meaning, 
and usage of this statement is identical to that described for JOVE.</P>

<P>When ZEUS is invoked by execution of the FIND statement, it activates 
derivative evaluation to compute gradients with respect to the unknowns.</P>

<P><i>The Solution Process</i> -  As noted above, the 
solution process is identical to that of JOVE, except that a Davidon-Fletcher-Powell 
(DFP) technique is used to select a direction for performing each 
of the sequence of unconstrained suboptimization. The ZEUS controls 
are also the same as those for JOVE, except that the SEARCH switch 
has no significance.</P>

<A NAME="ZEUS_CTL" ID="ZEUS_CTL"><P><B><i>ZEUS Controls</i></B></A> - The control variable for ZEUS are as follows. Note: In 
these descriptions, the term "iteration" refers to a complete unconstrained 
optimization.</P>
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>
<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detail iteration point </TD><TD>1</TD></TR>
<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary </TD><TD>1</TD></TR>
<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>

<TR><TD><B>REMAX</B></TD><TD></TD><TD>Maximum number of iterations</TD><TD>5</TD></TR>

<TR><TD><B>STEPSLIM</B></TD><TD></TD><TD>Limit on the number of steps per 100 iteration. 
                                         Since an iteration amounts to an entire 
                                         unconstrained optimization, this control 
                                         is analogous to REMAX for solver HERA. 
                                         However, if solving a constrained problem, 
                                         exceeding LIMSTEPS does not  terminate 
			                             the process, it merely halts the current iteration.</TD><TD></TD></TR>

<TR><TD><B>STEPOUT</B></TD><TD>n</TD><TD>Provides snapshot prints of the step-by-step 
                                         solution progress at every nth step of all iterations
			                             (about 4-9 lines of critical problem data).</TD><TD>5</TD></TR>
<TR><TD></TD><TD>-n</TD><TD>Constraint values are added to the snapshot prints.</TD><TD></TD></TR>			
<TR><TD></TD><TD>0</TD><TD>No snapshot prints are generated.</TD><TD></TD></TR>

<TR><TD><B>EVALMAX</B></TD><TD></TD><TD>Limit on total number of model evaluations 
                                        The optimization process is halted when this limit
	                                    is exceeded and the result is flagged in the same
			                            manner as  if maximum iterations were reached.
			                            The actual number of evaluations may exceed
			                            EVALMAX slightly because of the need to terminate
			                            the solution process correctly.</TD><TD>1000</TD></TR>
			
<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current 
                                       iteration is complete. (This control is only
                                       effective when a model abort is requested by
                                       an abort statement.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the 
                           current step in which an ABORT 
			               statement was executed.</TD><TD></TD></TR>

<TR><TD><B>SETRHO</B></TD><TD>1</TD><TD>Set the initial value of &rho; to 1</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD>Compute an initial value of <i> &rho;</i> which minimizes
			               the magnitude of the  gradient at the initial point.</TD><TD></TD></TR>
<TR><TD></TD><TD>3</TD><TD>Compute an initial value of <i> &rho;</i> which minimizes 
			(<i>&part;P/&part;x)(&Delta;x)</i> at the initial point.</TD><TD></TD></TR>
<TR><TD></TD><TD>n</TD><TD>(0&lt;&lt;n&lt;&lt;1) set value of <i> &rho;</i> to n (necessary for 
			               restarting a problem from a previously 
			               computed point - the appropriate value for <i> &rho;</i>
			               is printed in the detailed report.)</TD><TD></TD></TR>                           	
<TR><TD><B>RATIO</B></TD><TD></TD><TD>Factor used to reduce &rho; between successive 
                                      unconstrained optimizations: must be &gt; 2</TD><TD>16<BR></TD></TR>
			

<TR><TD><B>SPEEDUP</B></TD><TD>1</TD><TD>Do not accelerate by performing extrapolation.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD>Perform first order extrapolation.</TD><TD></TD></TR>
<TR><TD></TD><TD>3</TD><TD>Perform second order extrapolation.</TD><TD></TD></TR>

<TR><TD><B>ESTIMATE</B></TD><TD>1</TD><TD>Determine final problem convergence on the basis of 
                                          the current (zero order) solution.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD>Base convergence upon either the zero order
			               solution or on a first order extrapolation.</TD><TD></TD></TR>
<TR><TD></TD><TD>3</TD><TD>Base convergence upon either the zero order
			               solution or on a second order extrapolation.</TD><TD></TD></TR>

<TR><TD><B>CONVERGE</B></TD><TD>1</TD><TD>Assume that the problem finally converged when 
                                          the scaled objective functions of the primal and 
			                              dual solutions differ by less than ACCURACY.</TD><TD>1</TD></TR>
<TR><TD></TD><TD>2</TD><TD>Converge when the penalty contribution of the
			               inequality constraints is less  than ACCURACY.</TD><TD></TD></TR>			
<TR><TD></TD><TD>3</TD><TD>Converge when the scaled first order estimate<BR>
			               of the optimum objective differs from the dual<BR>
			               objective by less than  ACCURACY.</TD><TD></TD></TR>

<TR><TD><B>ACCURACY</B></TD><TD></TD><TD>Final convergence tolerance</TD><TD>10<SUP>-4</SUP></TD></TR>

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Iteration improvement limit; an unconstrained optimization 
                                        is terminated if the objective fails to improve by at least 
                                        this amount.</TD><TD>0</TD></TR>
			
<TR><TD><B>SUBTEST</B></TD><TD>1</TD><TD>Terminate an iteration (an unconstrained optimization) 
                                         when <i>(&part;P/&part;x<SUB>i</SUB>)</i> &lt; ZERO 
                                         for all <i> x<SUB>i</SUB></i>.</TD><TD>1<BR></TD></TR>
<TR><TD></TD><TD>2</TD><TD>Terminate an iteration when (<i>&part;P/&part;x<SUB>i</SUB></i>)
                           (&Delta;x<SUB>i</SUB>) is less than 20% of the improvement in the 
                           augmented objective for all <i>x<SUB>i</SUB><BR></TD><TD></TD></TR>			
<TR><TD></TD><TD>3</TD><TD>Terminate an iteration when |&part;P<D>/&part;x<SUB>i</SUB>| &lt; ZERO
			               for all <i>x<SUB>i</SUB>.</TD><TD></TD></TR>
			
<TR><TD><B>ZERO</B></TD><TD></TD><TD>Iteration convergence tolerance </TD><TD>10<SUP>-6</SUP></TD></TR>

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints </TD><TD>0<BR></TD></TR>
<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			
<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER </TD><TD>+1<BR></TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL </TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>
			
<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1<BR></TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL<BR></TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>
</TABLE>			
<HR>

<H2>7.3. Solver THOR </H2>
<P>THOR applies a "sectionally linearized"  linear programming technique 
to the solution of constrained optimization problems.  Where the constraints 
and objective are linear, a modified simplex algorithm is applied.  Where 
they are nonlinear, a sequence of linear programming problems is generated 
in which the model is locally linearized.  This is accomplished in 
such a way that the solutions of the linearized subprograms converge 
to the true solution of the nonlinear problem.  There is no requirement 
to identify which circumstance holds, as THOR automatically detects 
which technique is appropriate.</P>

<P>For true linear programming problems, THOR is the preferred solver. 
As they become progressively more nonlinear, either of JOVE, ZEUS, 
or JUPITER may be expected to give better performance.</P>  

<A NAME="FIND_THOR" ID="FIND_THOR"><P>The FIND statement for THOR takes the following form:<BR><BR></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>model</i><B; 
BY THOR </B>{<B>(</b><i>controller</i><B>)</B>}<B>; </B> <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>}  {<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</b>} {<B>WITH FLAG</B> <i>signal</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</b>} {<B>MATCHING</B> <i>equalities</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>WITH BOUNDS</B> <i>limits</i><B>;</B>}  <B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR><BR>              

Except for the BY <i>solver</i> clause, which invokes THOR, and the 
use of BOUNDS, the form, meaning, and usage of this statement are 
identical to that of JOVE.  (The BOUNDS parameters are described below.)</P>

<P>When THOR is invoked by executing a FIND statement, it activates derivative 
evaluation to compute gradients with respect to the <i>unknowns</i>.</P>

<P>Unlike JOVE and ZEUS, THOR cannot be used to solve unconstrained optimization 
problems.  This limitation is not particularly serious, however, since 
it is almost always possible, and usually necessary, to establish 
some inequality constraints to assure a physically reasonable solution.</P>

<P><B><i>The Solution Process </i></B> -  THOR's fundamental optimization 
technique is the revised simplex method.  Constraints are incorporated 
by means of slack variables and the problem is linearized in the vicinity 
of the current approximation point.  As each subproblem is solved, 
the problem is re-linearized and the allowable changes in the unknowns 
are bounded to minimize nonlinearity error.  For highly nonlinear 
problems, this may limit these changes in a way which impedes progress 
to a solution.  Although this may result in many more iterations, 
the overall efficiency of the technique provides a compensating factor.</P>

<P>Equality constraints are accommodated in a special manner by THOR. 
Rather than incorporating them directly, each is decomposed into two 
inequality constraints, i.e.,<BR>

&nbsp;&nbsp;&nbsp;&nbsp;<i>h(x<SUB>1</SUB>, ...,x<SUB>n</SUB>) = 0</i><BR>

becomes<BR>

&nbsp;&nbsp;&nbsp;&nbsp;h<SUB>1</SUB>(x<SUB>1</SUB>, ...,x<SUB>n</SUB> ) + &delta;  &ge 0 
    and   &delta; - h<SUB>2</SUB>(x<SUB>1</SUB>, ..., x<SUB>n</SUB>) &ge;  0 
<P>This approach, which is required by the general solution process, 
is less effective than the penalty function methods of JOVE and ZEUS, 
and far less effective than using AJAX to satisfy equality constraints (When 
equality constraints are required, it is essential (for THOR) to provide 
an initial starting point which satisfies these constraints.  If it 
is difficult to arrive at approximate values for the independent variables, 
because of complex relationships, then the technique described in 
Section 6.2.1 may be used.)</P>

<P>Many problems permit an error bound or range over which a constraint 
may be satisfied.  For example, a temperature may be constrained to 
70&plusmn;5<SUP>o</SUP>F.  For such cases, it is better to impose 
two inequality constraints than to force the solution to satisfy the 
equality constraint 70 &plusmn; &delta;<SUP>o</SUP> F, since &delta, as chosen 
by THOR, may be a physically unreasonable limitation.</P>

<P><B><i>Bounding</i></B>  -  Since the problem is only locally linearized, 
it is necessary to limit the calculated changes in the unknowns as 
the successive approximations are performed.  An upper limit is provided 
by the BOUNDS parameters.  This is a list of variables whose values, 
at the time at which the FIND statement is executed, prescribe the 
maximum allowable changes in each unknown during any iteration.  The 
specification of BOUNDS may take the same forms as described for the 
analogous parameters of HERA (see Section 6.1).  In the case of THOR, 
however, the use of the FRACT control is discouraged because the bounding 
is far more critical for this technique.  Reasonable values for bounds 
are approximately 10 percent of the total expected change in the associated 
unknowns.</P>

<P>THOR also applies "adaptive move limits" which actually compute the 
nonlinearity error and restrict the changes in the unknowns to values 
which guarantee no error greater than a prescribed value (given by 
the control ERRMAX).  This operation may be disabled by resetting 
the control variable ADAPT, but this usually is poor practice.  If 
nonlinearity error is so large that the adaptive limits are unacceptably 
small, an alternate solver should be used or a different initial estimate 
should be given.  When adaptive limits are being used, the upper limits 
on the changes, as specified by the BOUNDS, are automatically doubled 
in order to permit the maximum change consistent with the objective 
function.</P>

<P><B><i>Convergence </i></B> -  Detection of an optimum may occur in either 
of two ways, depending upon whether or not a true local extremum exists 
within the feasible region.  If the real optimum lies outside of this 
region, the solution to the problem lies on the constraint boundary 
at a point closest to the infeasible exterior optimum.  In this sense, 
"closest" means "as measured along the gradient."  This condition 
is detected by the fact that the computed change (step size) for one 
or more unknowns was less than its current limit.  On the other hand, 
if the optimum is accessible, the solution process uses the measured 
changes in the objective to progressively reduce the step size limits 
until they fall below a prescribed criterion (UNKNOWN) or until the 
objective improvement is acceptably small (PROGRESS).</P>

<P>The method is as follows.  Changes in the objective function are classified 
as "small" or "large" according to whether they are less than or greater 
than a discriminator PROGTEST.  When the number of consecutive small 
moves equals a control called STEPLIM, the step size limits are halved; 
if the number of consecutive large moves equals STEPLIM, the limits 
are doubled (up to a maximum initially set by BOUNDS).  There are 
two alternate convergence criteria which may be applied.  The objective 
convergence criterion measures the relative change in the objective 
against the measure PROGRESS.  If the change is smaller than PROGRESS, 
then the criterion in satisfied.  The <i>unknowns convergence</i> 
criterion measures the current step size limits against the originally 
specified bounds for each unknowns.  If all are less than UNKNOWN, 
then the criterion is satisfied.  Either or both criteria may be activated 
by the control variable CONVERGE.</P> 

<A NAME="THOR_CTL" ID="THOR_CTL"><P><B><i>THOR Controls </i></B></A> -  The control variables for THOR are as follows (An 
iteration is defined to be a complete linear programming subproblem.)</P>
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>
<TR><TD><B>ERRMAX</B></TD><TD></TD><TD>Maximum permitted nonlinearity error,
                                       measured by the current  values of 
			                          |&part;<SUP>2</SUP><i>f</i>/&part;x<SUB>i</SUB><SUP>2</SUP>| 
                                      for all <i>x<SUB>i</SUB></TD><TD>0.05</TD></TR>
			

<TR><TD><B>ADAPT</B></TD><TD>1</TD><TD>Apply adaptive control to hold 
                                       nonlinearity error below ERRMAX.</TD><TD> 1</TD></TR>
			
<TR><TD></TD><TD>0</TD><TD>Disable adaptive control.</TD><TD></TD></TR>            
			

<TR><TD><B>UNKNOWN</B></TD><TD></TD><TD>Unknowns convergence criterion; an 
                                        interior optimum has been reached  when
                            			|&Delta;x<SUB>I</SUB>/<i>b<SUB>I</SUB>| &le; UNKNOWN. for all 
                                        <i>x<SUB>i</SUB>, where <i>b<SUB>i</SUB>
			                            are  the initial step size bounds.</TD><TD>0.05</TD></TR>
			

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Objective convergence criterion; 
                                         an interior optimum has been reached 
			                             when |&Delta;f/f| &le; PROGRESS.</TD><TD>0.01</TD></TR>
			
<TR><TD><B>PROGTEST</B></TD><TD></TD><TD>Relative discriminator for determining if a 
                                         change in the objective is large or small; 
			                             measured against |&Delta;f/f|</TD><TD>0.01</TD></TR>
			

<TR><TD><B>STEPLIM</B></TD><TD></TD><TD>Number of consecutive large or small 
                                        moves required before changing the
			                            step size limits</TD><TD>3</TD></TR>
			
<TR><TD><B>FRACT</B></TD><TD></TD><TD>Fractional bound to be applied to all 
                                      independent variables</TD><TD>0.5</TD></TR>
			

<TR><TD><B>CONVERGE </B></TD><TD>1</TD><TD>Satisfy objective or unknowns convergence. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Satisfy objective and unknowns convergence.</TD><TD></TD></TR>
			

<TR><TD><B>REMAX </B></TD><TD></TD><TD>Maximum number of allowed iterations</TD><TD>40</TD></TR>

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration print.</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration 
			               plus first and last</TD><TD></TD></TR>
		 	
<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER </TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>			
			
<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER </TD><TD>-1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL</TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>			
			
<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoints after every nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			
<TR><TD><B>RESET</B></TD><TD>1</TD><TD>Decrease the step size and restore 
                                       the previous iteration values whenever
			                           &Delta;f reverses sign.</TD><TD>0</TD></TR>

<TR><TD></TD><TD>0</TD><TD>Decrease the step size but continue
			               from the current values whenever
			               &Delta;f reverses sign.X</TD><TD></TD></TR>			
</TABLE>
<HR>			
<P>For the majority of fairly linear problems, the preset controls should 
prove acceptable.  For cases in which this is not true, an examination 
of the iteration and summary prints usually suggests how controls 
may be reset for greater effectiveness.  In general, better convergence 
properties are achieved with adaptive error control and with STEPLIM 
=3 or 4.  For PROGTEST much greater than 0.05, spurious convergence 
at a suboptimal point may occur.  As a guideline for these controls:</P>

<TABLE>
<TR><TD>Large PROGTEST and Small STEPLIM</TD><TD>Fast convergence but some danger of selecting a suboptimal solution.</TD></TR>

<TR><TD>Small PROGTEST and Small STEPLIM</TD><TD>Quick approach to the vicinity of the optimum but slow convergence thereafter.</TD></TR>

<TR><TD>Small PROGTEST and Large STEPLIM</TD><TD>Stable but relatively slow convergence</TD></TR>
</TABLE>


<P>The accuracy of the solution is directly related to the control UNKNOWN.  The 
smaller its value, the more accurate the converged objective.  There 
is a price paid for increased accuracy, however, by an increase in 
the number of iterations required.  As a rough guide, a second order 
of magnitude improvement in accuracy will increase the number of iterations 
by 30 percent.</P>

<P>The control switch RESET is mainly useful when the initial estimate 
is known to be close to the desired optimum, for example when the 
last of a sequence of progressive optimizations is being performed. 
Turning this switch on will prevent any serious overshoot.</P>

<P><B><i>Optimization Summary Report</i></B>  -  A standard optimization report 
is nominally generated by THOR.  In format and content, it is identical 
to that produced by JOVE (see Figure 7-1).  It should be recalled 
that each iteration is a complete linear programming problem.  Thus, 
when the objective and the constraints are linear, it will converge 
in a single iteration.</P>

<P><B><i>Detailed Iteration Report</i></B>  -  A detailed iteration report is 
issued by THOR whenever DETAIL is nonzero.  It displays the values 
of the objective, the unknowns, and the constraints and gives information 
which indicates how step size limits are currently progressing.  When 
adaptive limits are introduced or backtracking occurs under the RESET 
option, a suitable notation is printed.</P>

<H2>7.4. Solver JUPITER </H2>

<P>JUPITER is based upon an algorithm called COMET, contributed 
by Professor D. M. Himmelblau, and documented by R.L. Staha, Dept. 
of Chemical Engineering, the University of Texas at Austin. IT may 
be applied to solve either constrained or unconstrained optimization 
problems, and trial usage has shown it to be generally superior in 
speed, accuracy, and memory requirements to any of the previously 
described solvers.  For unconstrained problems, the solution technique 
is the Davidon-Fletcher-Powell (DFP) variable-metric method.  This 
is a first order method, i.e., one requiring only first partial derivatives 
in the model.</P>

<A NAME="FIND_JUPITER" ID="FIND_JUPITER"><P>The general form of the FIND statement for JUPITER is</P></A> 

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>model</i><B>; BY JUPITER</B>{<B>(</b><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>}{{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {<B>WITH FLAG</B> <i>signal</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>AND</B>} <B>HOLDING</b> <i>inequalities</i><B>;</B>}{{<B>AND</B>} <B>MATCHING</B> 
<i>equalities</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR><BR>

Except for the BY solver clause, which invokes JUPITER, the form, 
meaning, and usage of this statement are identical to that of JOVE 
(Section 7.1).<br>

<IMG src="../images/JupMethod.png" height="887" width="768"> <br>[FIX need to 
recreate equations without image]<A NAME="JUPITER_CTL" ID="JUPITER_CTL"><P><B><i>JUPITER Controls</i></B></A>  -  JUPITER has the following control variables:</P> 

<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete. <BR></TD><TD>1<BR></TD></TR>
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the <BR>
			current step in which an ABORT <BR>
			statement was executed.</TD><TD></TD></TR>			
			

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration print</TD><TD>1<BR></TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration <BR>
			plus first and last</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary </TD><TD>1<BR></TD></TR>
<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>REMAX</B></TD><TD></TD><TD>Maximum number of allowed iterations</TD><TD>20</TD></TR>
<TR><TD><B>STEPSLIM</B></TD><TD></TD><TD>Limit on the number of steps per iteration Since an iteration amounts to an entire<BR>
			unconstrained optimization, this control is <BR>
			analogous to REMAX  for solver HERA. <BR>
			However, if solving  a constrained problem, <BR>
			exceeding STEPSLIM does not terminate <BR>
			the process, it merely halts the current iteration.</TD><TD>250<BR></TD></TR>
			
<TR><TD><B>STEPOUT</B></TD><TD>n</TD><TD>Provides snapshot prints of the step-by-step solution progress at every nth step of all iterations<BR>
			 (about 4-9 lines  of critical problem date).<BR></TD><TD>-5<BR></TD></TR>

<TR><TD></TD><TD>-n</TD><TD>Constraint values are added to the snapshot prints.<BR></TD><TD></TD></TR>			

<TR><TD></TD><TD>0</TD><TD>No snapshot prints are generated.</TD><TD></TD></TR>			
			

<TR><TD><B>EVALMAX</B></TD><TD></TD><TD>Limit on the total number of model evaluations The optimization process is halted when this <BR>
			limit is exceeded and the result is flagged in <BR>
			the same  manner as if maximum iterations <BR>
			were  reached.  The actual number of evaluations<BR>
			may exceed EVALMAX slightly because of the <BR>
			need to terminate the  solution process correctly.</TD><TD>1000<BR></TD></TR>
			

<TR><TD><B>ZERO</B></TD><TD></TD><TD>Iteration convergence tolerance</TD><TD>10<SUP>-6</SUP></TD></TR>

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0<BR></TD></TR>
<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER </TD><TD>+1<BR></TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL <BR></TD><TD></TD></TR>
<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>			
			 

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER </TD><TD>+1<BR></TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL <BR></TD><TD></TD></TR>
			

<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>
			

<TR><TD><B>ACCURACY</B></TD><TD></TD><TD>Relative tolerance for constraint satisfaction at convergence. 
           This variable must be greater<BR>
			than  control variable ZERO.</TD><TD>0.001<BR></TD></TR>
			

<TR><TD><B>RHO</B></TD><TD></TD><TD>Weighting factor for the penalty function during the initial iteration</TD><TD>0.02<BR></TD></TR>
</TABLE>			
<HR>

<H2>7.5. Solver HERCULES </H2>

<P>HERCULES is a special constrained optimization solver for linear, 
integer, and mixed-integer problems. Unlike the general solvers it 
can only be applied to models, which contain linear relationships.</P>

<P>The restrictions upon HERCULES can best be described by contrasting 
it with the typical general solver, such as THOR or JOVE.  The key 
differences are:</p>

<ol type="1">
<li>The function to be optimized must be a linear function 
of the independent variables. Thus, if x is a vector of independent  variables, 
then the objective function must take the following form:<BR> 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>	f = c<SUB>0</SUB> + c<SUB>1</SUB> + c<SUB>2</SUB>x<SUB>2</SUB> 
+ ... + c<SUB>n</SUB>x<SUB>n</SUB></i><BR>

where <i>c<SUB>0</SUB></i> and <i>c<SUB>1</SUB></i> are constants, 
not all of which are zero.<BR></li>

<li>There must be at least one constraint and all constraints 
must be linear functions of the independent variables.  In vector  notation, 
the constraints must take the form:<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i> <B>a x</B></i>   &ge;   <B><i>b</i></B><BR>

where some or all may be equalities or inequalities.</li>

<li>The independent variables must be non-negative and, in fact, no lower clause is permitted in the FIND statement.  Thus, the  additional 
limits may be expressed as:<BR>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<i> &le; <B>x</i></B>  &le; <B>u</B><BR>

where the constants <MIO>u<D> are optional upper limits 
on the independents.</li>
</ol>

<P> Special capabilities of HERCULES include:</P>

<UL>
<LI><i>Integer and Mixed Integer Programming</i> - any or all 
of the  independent variables may be restricted to integer values, 
and in fact, even to a limited subset of the integers.</LI>

<LI><i>Discrete Objectives</i> - The objective function may 
be limited to  integral values.</LI>

<LI><i>Zero-one Programming</i> - The special case of integer 
programming  in which all variables take on only the values zero and 
one can  be handled.  The classical assignment problem is a case in  point.</LI>

<LI><i>Restart Feature</i> - A checkpoint may be requested on 
any  premature termination.  The problem may then be completed from  this 
point without repeating previous calculations.</LI>

<LI><i>Problem Size Control</i> - Several techniques are used 
to permit  very large problems to be solved.  There is no predefined 
limit  on the number of variables or equations.</LI>
</UL>

<A NAME="FIND_HERCULES" ID="FIND_HERCULES"><P>The general FIND statement for HERCULES is:</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>model</i><B>;  BY 
HERCULES</B>{<B>(</b><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</b><i> inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>} <B>UPPERS</B> <i>ceiling</i>;} {{<B>WITH</B>|<B>AND</B>} 
<B>DISCRETES</b> <i>steps</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>FLAG</B>  <i>signal</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B>  <i>objective</i><BR>

<P>The meaning and usage of all elements of this statement are identical 
to those described for JOVE in section 7.1, with the exception of 
the new DISCRETES clause.</P>

<P>The DISCRETES clause identifies unknowns which are to be restricted 
to integral values and further specifies their step sizes.  If this 
clause is present, the list <i>steps</i> must contain an entry for 
every element of the<i> unknown</i>s.  Any entry with a value of zero 
is ignored and the corresponding unknown is free to take on any feasible 
continuous value.  A discrete variable with step size <i>n</i> can 
only take values which are integral multiples of <i>n</i> and HERCULES 
will restrict its search accordingly.  Thus, if there are 3 independent 
variables and the steps are specified by a vector called STP, then 
the following initialization will have the given consequences :<BR>
<ol type="a">
<li>DATA STP /0,0,1/<BR>

only X(3) will be discrete and its step size will be 1;</li>

<li>DATA  STP/10,10,10/<BR>

all unknowns are discrete and all have step sizes of 10.</li>
</ol>

<P>Obviously, it is not essential to limit a discrete variable to non-unit 
steps, but it is highly desired to do so if possible.  For one thing, 
scaling problems in defining the constraints will be reduced, thereby 
eliminating a potential for numerical difficulty.</P>

<P>The flag variable <i>signal</i>, if present, will be used to signal 
whether the solution was successful or not.  It will be zero unless 
the solution process terminated prematurely because an iteration, 
recheck, or storage limit was exceeded.</P>

<P>The way in which the model is used in the solution process differs 
from the other optimization solvers. Even though HERCULES employs 
an iterative solution technique, it does not call the model at all 
during optimization.  Instead, it calls it only during initialization 
and after the solution is achieved.  The preliminary calls are used 
solely to extract information which defines the problem, i.e. the 
various linear coefficients of the constraints and the objective function.  During 
these calls, gradients may be computed on some or all of the unknowns.  The 
number of calls depends upon the available data storage space.  For 
small problems, this number will be in the range 2-10, for very large 
problems and medium-sized memory allocations, the number may go as 
high as 100. The number of calls may be minimized by providing a sufficiently 
large memory allocation for execution.  As guided by experience, the 
terminating call is used simply to recalculate the constraints using 
the converged values of the unknowns.</P>

<P>HERCULES cannot be employed while partial derivative evaluation is 
in progress and thus cannot be nested within another calculus process.</P>

<A NAME="HERCULES_CTL" ID="HERCULES_CTL"><P><B><i>Control Variables</i></B></A> - The control variables for HERCULES are 
listed below:</P>
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>TYPE</B></TD><TD>0</TD><TD>Objective may be any real value</TD><TD>0</TD></TR>
<TR><TD></TD><TD>1</TD><TD>Objective is restricted to integer values
			If the computation of the objective will 
			necessarily give an integral value except 
			for possible round-off errors, do not use 
			control variable TYPE. This control is 
			necessary only for an  integral solution to a 
			continuous or mixed integer problem.<B></TD><TD></TD></TR>
			

<TR><TD><B>DISCRETE</B></TD><TD>0</TD><TD>All unknowns are continuous variables </TD><TD>0</TD></TR>

<TR><TD></TD><TD>1</TD><TD>All unknowns are integer variables 
			with unit step size
			This control provides a simple alternative 
			to using the DISCRETES clause whenever 
			all unknowns are discrete and their step 
			size is one.  It is ignored if a DISCRETE 
			clause is present in the FIND statement.  </TD><TD></TD></TR>
			

<TR><TD><B>ONEZERO</B></TD><TD>0</TD><TD>All unknowns have no upper bound </TD><TD>0</TD></TR>

<TR><TD></TD><TD>1</TD><TD>All unknowns have an upper bound of 1
			This control provides a simple alternative
			to using the uppers clause whenever all
			unknowns have an upper bound of one.  
			It is ignored if an uppers clause is present 
			in  the FIND statement. </TD><TD></TD></TR>
			

<TR><TD><B>REPORT</B></TD><TD>0</TD><TD>Suppress solver solution report. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Generate solution report.</TD><TD></TD></TR>
			 

<TR><TD><B>REPOUT</B></TD><TD>-1</TD><TD>Route solution report to Console.</TD><TD>-1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>Route solution report to PRINTER file.</TD><TD></TD></TR>
			

<TR><TD></TD><TD>+1</TD><TD>Route solution report to SCROLL file.</TD><TD></TD></TR>        
			 

<TR><TD><B>SIZE</B></TD><TD>0</TD><TD>Set problem size to maximum. </TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Use this value as the problem size.
			Specifies an estimate of the problem size
			in terms of the maximum number of 
			<i>effective</i> constraints. (see discussion
			of problem sizing below.)</TD><TD></TD></TR>
			

<TR><TD><B>STEPSLIM</B></TD><TD></TD><TD>Linear program iteration limit</TD><TD>0</TD></TR>

<TR><TD></TD><TD>0</TD><TD>Use  (N<SUB>c</SUB>+N<SUB>u</SUB>+N<SUB>b</SUB>)*3  where
				N<SUB>c</SUB> = total number of constraints <BR>
				N<SUB>u</SUB> = number of unknowns<BR>
				N<SUB>b</SUB> = number of upper bounds<BR>
</TD><TD></TD></TR>

<TR><TD></TD><TD>n</TD><TD>Use this value as the limit.</TD><TD></TD></TR>
        	

<TR><TD><B>REMAX</B></TD><TD></TD><TD>Integer / mixed integer program iteration limit</TD><TD>0</TD></TR>

<TR><TD></TD><TD>0</TD><TD>Use N<SUB>u</SUB>*STEPSLIM per above symbols</TD><TD></TD></TR>
			

<TR><TD></TD><TD>n</TD><TD>Use this value as the limit in integer problems,
			the linear continuous problem is executed in
			a nested fashion.  REMAX imposes an upper 
			limit on the cumulative linear program iterations.
			Thus, there is no point in setting REMAX less than
			STEPSLIM.</TD><TD></TD></TR>
			

<TR><TD><B>RECHECK</B></TD><TD></TD><TD>Re-inversion limit 
            If the solver is unable  to achieve the 
			prescribed objective accuracy and 
			constraint tolerance, a new inverse 
			is computed. Periodic re-inversions 
			help to quench accumulated numerical 
			errors but excess re-inversions probably 
			indicate unreasonable small tolerances.</TD><TD>5</TD></TR>
			

<TR><TD><B>SAVE</B></TD><TD></TD><TD>Restart file designator</TD><TD>0</TD></TR>

<TR><TD></TD><TD>0</TD><TD>Do not generate a restart file </TD><TD></TD></TR>
		 	

<TR><TD></TD><TD>n</TD><TD>Write a restart file to ligical unit n, 
			i.e. per FORTRAN ... WRITE(n) LIST 
			n must be an integer in the range 1-50.
			See discussion of the restart feature below.</TD><TD></TD></TR>
			

<TR><TD><B>ACCURACY</B></TD><TD></TD><TD>Absolute objective tolerance
            Sets the level of discrimination between 
			computed values of the objective function.
			If too low, the solver may iterate unnecessarily.</TD><TD>10<SUP>-5</SUP></TD></TR>
			

<TR><TD><B>EPSILON</B></TD><TD></TD><TD>Absolute constraint tolerance,
            measures the degree to which constraints 
			may be violated. If too low, the solver may 
			iterate to the limit. </TD><TD>10<SUP>-5</SUP></TD></TR>
			

<TR><TD><B>DELTA</B></TD><TD></TD><TD>Absolute unknowns tolerance,
            measures the degree to which the unknowns 	
			may violate the upper and lower (0) limits. </TD><TD>10<SUP>-6</SUP></TD></TR>
			

<TR><TD><B>DELINT</B></TD><TD></TD><TD>Integer test tolerance
Values are considered to be integers if they 
			differ by less than DELINT from a true integer. 
</TD><TD>10<SUP>-6</SUP></TD></TR>
			
<TR><TD><B>ROUNDOFF </B></TD><TD></TD><TD>Round-off error tolerance
Computed values less than roundoff are 
			assumed to be zero.  A value too small will 
			generate large error, resulting in numerous 
			re-inversions.  Too large a value may result 
			in failure to converge because valid values 
			are ignored. 
</TD><TD>10<SUP>-5</SUP></TD></TR>
</TABLE>
<HR>			




<P><i>Zero-One Programming</i> - The standard zero-one integer programming 
problem is specified  by setting both DISCRETE and ONEZERO to one 
and omitting both  UPPERS and DISCRETES clauses from the FIND statement.  Be  careful 
to give meaningful combinations of these controls and  clauses.</P>

<P><i>Solution Report</i> -  The solution report generated by HERCULES 
essentially summarizes  the initial conditions and controls and shows 
the final values of the objective, unknowns, and constraints.  HERCULES  automatically 
scales the constraints for the purpose of  minimizing computational 
error and those displayed incorporate  the scale factors.  Values 
of the program variables may be  printed separately to obtain the 
true constraint values.  The  report also shows the actual number 
of iterations and accuracy  rechecks required as well as the maximum 
problem size (see  below).  These are guides for subsequent settings 
of the control variables REMAX, RECHECK, and SIZE.</P>

<P><i>Problem size</i> - The maximum problem size is given by  SIZE = 
min(<i>m</i>,<i>n</i>)  where<i> m</i> = total number of equality 
and inequality constraints and <i>n</i> = number of unknowns.  Most 
problems will not require the maximum size, although only  experience 
can show what is needed.  This value is used to dimension some internal 
arrays used by HERCULES and an unnecessarily large value will require 
more memory allocation than will be used, and perhaps even more than 
is available for very large problems. The solution report shows the 
actual size employed, and this may be used as a guide in later runs 
of the same  or similar problems.  A reasonable estimate for medium 
sized  problems is about 75 percent of the maximum value.</P>

<P><i>Tolerance Controls</i> - Settings of the tolerance controls require 
some judgment and expertise.  However, experience has shown the default 
values to be acceptable for a wide range of problems.  If any of the 
related variables have  conspicuously large or small values (say outside 
of the rough range 10<SUP>-3</SUP> to 10<SUP>+3</SUP>), the defaults may not 
be valid.  It is desirable, if not to inconvenient, to scale the model 
variables to about order unity.  The number of accuracy rechecks in 
the solution report is an  indicator of the validity of the chosen 
tolerance controls.  If  this number is more than 2 or 3, some tolerances 
are probably too tight.</P>

<P><i>Checkpointing a Problem Run</i> - If SAVE is nonzero, a solution 
checkpoint is written to this file number in any circumstance in which 
a converged solution is not achieved.  This can be due to reaching 
an iteration or recheck limit or alternatively to an apparently infeasible 
or unbounded problem.  The latter cases usually arise from improperly 
set tolerance  controls.  A final case can arise from exceeding the 
specified problem size.  To use this facility, the SAVE file must 
be opened and subsequently saved.</P>

<P><i>Restarting a Checkpointed  Run</i>  - To restart from a saved checkpoint, 
a special statement is used as follows:</P>



&nbsp;&nbsp;&nbsp;&nbsp;<B>RESUME(</B><i>file</i><B>)</B> <i> unknowns</i> <B>; BY HERCULES</B>{<B>(</B><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH </B>}<B> FLAG</B> <i>signal</i><B>; </B>}<B> TO OPTIMIZE </B><i>objective</i><BR>

<P>where <i>file</i> is the logical unit number of the file containing 
the checkpoint,<i> unknowns </i>is an array dimensioned at least as 
large as the originally defined independent variables of  the checkpointed 
problem, <i>controller</i> is a new controller procedure, <i>signal</i> 
and <i>objective</i> are output variables corresponding to the flag 
and objective variables of the checkpoint.</P>   

<P>This statement resumes the checkpointed FIND statement and does not 
require respecification of the model.  The output <i>unknowns</i>, 
<i>signal</i>, and <i>objective </i>may be used in subsequent computation 
or output statements as desired.</P>

<P>New control values may be specified in the controller procedure, including 
all but DISCRETE, INTEGER,  and ONEZERO.  These three cannot be changed 
without redefining the problem being solved.  <i>If any controls 
are not set in this manner, they take their default values, not the 
values in effect when the  checkpoint was taken.</i>   It is also  valid 
to create another checkpoint by setting an appropriate file number 
to SAVE.  If this number is the same as the original checkpoint, the 
original one will be overwritten.</P>

<H4>Examples</H4>

<P><i>Continuous Linear Programming</i> - The first example is a feed-blending 
optimization.  It is desired to blend ingredients in the least expensive 
way such than the stated restrictions are satisfied:</P>

<TABLE>
<TR><TH>Ingredient</TH> 	<TH>Protein</TH> 	<TH>Fat</TH> 		<TH>Fiber</TH> 		<TH>Cost/lb</TH></TR>

<TR><TD>Screenings</TD>	<TD>0.130</TD>		<TD>0.030</TD>		<TD>0.070</TD>		<TD>$0.022</TD></TR>            

<TR><TD>Barley</TD>		<TD>0.115</TD>		<TD>0.020</TD>		<TD>0.060</TD>		  <TD>0.025</TD></TR>              

<TR><TD>Corn</TD>		<TD>0.086</TD>		<TD>0.038</TD>		<TD>0.025</TD>		  <TD>0.028</TD></TR>               

<TR><TD>Salt</TD>		   <TD>-</TD>		   <TD>-</TD>		   <TD>-</TD>  			 <TD>0.011</TD></TR>            
</TABLE>
  

<H5>Restrictions:</H5>
<OL Type="a">
<LI>Total weight of blend = 2000 lb.</LI>
<LI>Total protein &ge; 200 lb.</LI>
<LI>Total fat &ge 54 lb.</LI>
<LI>Total fiber &le; 90 lb.</LI>
<LI>Amount of corn between 400 and 1000 lb.</LI>
<LI>Weight of salt used = 5 lb.</LI>
</OL>

The program is as follows:<BR>

<PRE>
      GLOBAL ALL
      PROGLEM BLEND
        SALT=5 : BARLEY=500 : CORN= 900 : SCREENS=595
        FIND BARLEY,CORN,SCREENS; IN MIX; BY HERCULES;
     ~     HOLDING PROTEIN, FAT, FIBER, CORNLOW, CORNHIGH;
     ~      AND MATCHING WEIGHT; TO MINIMIZE COST
      END

      MODEL MIX
        WEIGHT=BARLEY+CORN+SCREENS+SALT-2000
        PROTEIN=0.115*BARLEY+0.086*CORN+0.13*SCREENS-200
        FAT=0.02*BARLEY+0.038*CORN+0.03*SCREENS-54
        FIBER=90-0.06*BARLEY-0.025*CORN-0.07*SCREENS
        CORNLOW=CORN-400
        CORNHIGH=1000-CORN
        COST=0.025*BARLEY+0.028*CORN+0.022*SCREENS+0.011*SALT
      END
</PRE> 

The solution is:<BR>

 
<PRE>
      COST      $51.34
      BARLEY    465.00 LB
      CORN      1000.00 LB	
      SCREENS   530.00 LB
      SALT      5.00 LB
</PRE> 

<P><i>Mixed Integer Programming</i> - A slight variation fo the preceding 
example shows the consequence of imposing the reasonable restriction 
that the barley and corn ingredients can only be purchased in 50 lb. 
lots.  The problem then becomes a mixed integer problem as follows:</P>

<PRE>
      GLOBAL ALL
      PROBLEM BLEND2
        DIMENSION VLIMITS(3)
        DATA VLIMITS/50,50,0/
        SALT=5 : BARLEY=500 : CORN=900 : SCREENS=595
        FIND BARLEY,CORN,SCREENS; IN MIX BY HERCULES; 
     ~     WITH DISCRETE VLIMITS; MATCHING WEIGHT;
     ~     HOLDING PROTEIN,FAT,FIBER,CORNLOW,CORNHIGH;
     ~     TO MINIMIZE COST
      END
</PRE>
 

The Solution is now:<BR>

<PRE>
     COST       $54.45
     BARLEY     500 LB
     CORN       1000 LB
     SCREENS    495.00 LB
     SALT       5.00 LB
</PRE> 

<P><i>Integer Programming</i> - A third example is a classical transportation 
problem, in which it is desired to minimize the delivery cost in stocking 
three warehouses from seven distributed manufacturing plants.  The 
deliveries to the warehouses must be 700, 900, and 400  items per 
month respectively, while the plants have production capacities of 
100, 300, 400, 200,300, 300, and 400 items per month respectively.  This 
information is conveniently shown in a transportion cost matrix in 
the following manner, where the table entries show the cost of shipping 
one item from a given plant to a given warehouse:</P>

<H4> --- WAREHOUSE---</h4>	  		   
<TABLE WIDTH="50%">
<TR><TH ALIGN=LEFT>PLANT</TH> <TH ALIGN=LEFT>D1</TH> <TH ALIGN=LEFT>D2</TH>	 <TH ALIGN=LEFT>D3</TH>	<TH ALIGN=LEFT>CAPACITY</TH></TR>
<TR><TD>01</TD>		 <TD>3</TD>	 <TD>2</TD>	 <TD>4</TD>				<TD>100</TD></TR>
<TR><TD>02</TD>		 <TD>0</TD>	 <TD>4</TD>	 <TD>2</TD>				<TD>300</TD></TR>
<TR><TD>03</TD>		 <TD>2</TD>	 <TD>1</TD>	 <TD>3</TD>				<TD>400</TD></TR>
<TR><TD>04</TD>		 <TD>5</TD>	 <TD>2</TD>	 <TD>3</TD>				<TD>200</TD></TR>
<TR><TD>05</TD>		 <TD>1</TD>	 <TD>4</TD>	 <TD>0</TD>				<TD>300</TD></TR>
<TR><TD>06</TD>		 <TD>4</TD>	 <TD>1</TD>	 <TD>2</TD>				<TD>300</TD></TR>
<TR><TD>07</TD>		 <TD>1</TD>	 <TD>6</TD>	 <TD>4</TD>				<TD>400</TD></TR>
<TR><TD>Req/Month</TD>		 <TD>700</TD>	 <TD>900</TD>	 <TD>400</TD>				<TD></TD></TR>
</TABLE>

<P>This is coded as an integer programming problem using the following 
symbols:</P>

<TABLE>
<TR><TD>OD</TD>  	<TD>=</TD>	<TD>Origin-Destination delivery rates, to be determined</TD></TR>
<TR><TD>COD</TD>  	<TD>=</TD>	<TD>Transportation Cost Matrix</TD></TR>
<TR><TD>SC</TD>  	<TD>=</TD>	<TD>Source Capacities</TD></TR>
<TR><TD>DR</TD>  	<TD>=</TD>	<TD>Delivery Requirements</TD></TR>
<TR><TD>SCC</TD>  	<TD>=</TD>	<TD>Shipping Constraints, Deliveries cannot exceed capacities</TD></TR>
<TR><TD>DRC</TD>  	<TD>=</TD>	<TD>Delivery Constraints, Requirements must be matched</TD></TR>
</TABLE>

<H4>Program:</H4>

<PRE>
      GLOBAL ALL
      PROBLEM TRANSPRT
        DYNAMIC OD,COD,SCC,DRC
        ALLOT OD(7,3),COD(7,3),SCC(7),DRC(3)
        &lt;SC&gt;=DATA(100,300,400,200,300,300,400)
        &lt;DR&gt;=DATA(700,900,400)
        &lt;COD&gt;=DATA(3,0,2,5,1,4,1,2,4,1,2,4,1,6,4,2,3,3,0,2,4)
        FIND OD; IN SHIPPER; BY HERCULES(SETUP);
     ~     HOLDING SCC; AND MATCHING DRC; TO MINIMIZE COST
      END

      MODEL SHIPPER
        DYNAMIC ROL,COL,TEMP,SCC,DRC
        DO 10 I=1,7
          &lt;ROW&gt;=&lt;[OD],I,*&gt;
          SCC(I)=SC(I)-ARRAYSUM(ROW)
   10   CONTINUE
        DO 20 I=1,3
          &lt;OL&gt;=&lt;[OD],*,I&ge
          DRC(I)=DR(I)-ARRAYSUM(COL)
   20   CONTINUE
          &lt;TEMP&gt;=&lt;OD&gt;*&lt;COD&gt;
          COST=ARRAYSUM(TEMP)
      END

      CONTROLLER SETUP(HERCULES)
        DISCRETE=1
      END
</PRE>

<P>Note that OD is initialized (by default) to zero, which is clearly 
not feasible.  HERCULES can solve problems from infeasible initial 
conditions but they present a greater danger of convergence to a suboptimal 
value.</P>


<P>The solution is  COST = 18000, with a delivery rate matrix OD:</P>

<TABLE width="50%">
<TR><TH align=left></TH> 	<TH align=left>(1)</TH>	<TH align=left>(2)</TH>	<TH align=left>(3)</TH></TR>

<TR><TD>(1)</TD>	 <TD>0</TD>	<TD>100</TD>	 <TD>0</TD></TR>
<TR><TD>(2)</TD>	 <TD>300</TD>	<TD>0</TD>	 <TD>0</TD></TR>
<TR><TD>(3)</TD>	 <TD>0</TD>	<TD>400</TD>	 <TD>0</TD></TR>
<TR><TD>(4)</TD>	 <TD>0</TD>	<TD>200</TD>	 <TD>0</TD></TR>
<TR><TD>(5)</TD>	 <TD>0</TD>	<TD>0</TD>	 <TD>300</TD></TR>
<TR><TD>(6)</TD>	 <TD>0</TD>	<TD>200</TD>	 <TD>100</TD></TR>
<TR><TD>(7)</TD>	 <TD>400</TD>	<TD>0</TD>	 <TD>0</TD></TR>
</TABLE>

<P><i>Zero-One Programming</i> - The last example is a classical assignment 
problem, i.e. a zero-one integer programming problem.  The Lehigh 
Co. is assigning 5 production orders to its 5 departments so as to 
minimize the cost of production.  Each department must get exactly 
one job and all production orders must be filled.  The cost of production 
per department for each type of job is as follows:</P>

<H4> DEPARTMENT</H4>
<TABLE width="50%">
<TR><TH align=left>ORDER</TH>		   <TH align=left>A</TH>	   <TH align=left>B</TH>	   <TH align=left>C</TH>	   <TH align=left>D</TH>		   <TH align=left>E</TH></TR>        
<HR>
<TR><TD>1</TD>		<TD>$120</TD>	<TD>$150</TD>	<TD>$75</TD>	<TD>$90</TD>		<TD>$100</TD></TR>
<TR><TD>2</TD>		  <TD>140</TD>	    <TD>80</TD>	    <TD>90</TD>	  <TD>85</TD>	 	  <TD>170</TD></TR>
<TR><TD>3</TD>		    <TD>50</TD>	    <TD>40</TD>	    <TD>40</TD>	  <TD>70</TD>		  <TD>110</TD></TR>
<TR><TD>4</TD>		    <TD>75</TD>	    <TD>65</TD>	    <TD>45</TD>	  <TD>70</TD>	 	    <TD>90</TD></TR>
<TR><TD>5</TD>		  <TD>110</TD>	  <TD>909</TD>	  <TD>140</TD>	 <TD>115</TD>	  <TD>100</TD></TR>
</TABLE>

<P>The terminology of the program is:</P>
<TABLE>
<TR><TD>POD</TD>	<TD>=</TD>	<TD>Production Distribution Matrix, to be determined</TD></TR>

<TR><TD>CPOD</TD>	<TD>=</TD>	<TD>Production Cost Matrix, as shown above</TD></TR>

<TR><TD>PODLIM <TD>=</TD>	<TD>Distribution constraints, specifying that thesums of the rows
		and columns of POD  must all be 1</TD></TR>
</TABLE>


<H4>Program:</H4>
<PRE>
      GLOBAL ALL
      PROBLEM ASSIGN
        DYNAMIC POD,CPOD,PODLIM
        ALLOT POD(5,5),CPOD(5,5),PODLIM(10)
        &lt;CPOD&gt;=DATA(120, 140,  50,  75, 110,
     ~              150,  80,  40,  65,  90,
     ~                75,  90,  40,  45, 140,
     ~                90,  85,  70,  70, 115
     ~                100, 170, 110,  90, 100)
        FIND POD; IN LEHIGH; BY HERCULES(SETUP);
     ~     MATCHING PODLIM; TO MINIMIZE COST
      END

      MODEL LEHIGH
        DYNAMIC ROW,COL,TEMP
        DO 10 I=1,5
          [ROW]=&lt;[POD],I,*&gt;
          [COL]=&lt;[POD],*,I&gt;
          PODLIM(I)=1-ARRAYSUM(ROW)
          PODLIM(I+5)=1-ARRAYSUM(COL)
   10   CONTINUE
        &lelTEMP&gt;=&lt;POD&gt;*&lt;CPOD&gt;
        COST=ARRAYSUM(TEMP)
      END

      CONTROLLER SETUP(HERCULES)
        DISCRETE=1
        ONEZERO=1
      END
</PRE>
 

<P>The solution is:  COST = $365 via the distribution matrix POD:</P>

<TABLE width="50%">
<TR><TD> </TD> <TD>(1)</TD> <TD>(2)</TD>	<TD>(3)</TD>	<TD>(4)</TD>	<TD>(5)</TD></TR>
<TR><TD>(1)</TD> <TD>0</TD> <TD>0</TD>	<TD>0</TD>	<TD>1</TD>	<TD>0</TD></TR>
<TR><TD>(2)</TD> <TD>0</TD> <TD>1</TD>	<TD>0</TD>	<TD>0</TD>	<TD>0)</TD></TR>

<TR><TD>(3)</TD> <TD>1</TD> <TD>0</TD>	<TD>0</TD>	<TD>0</TD>	<TD>0</TD></TR>
<TR><TD>(4)</TD> <TD>0</TD> <TD>0</TD>	<TD>1</TD>	<TD>0</TD>	<TD>0</TD></TR>
<TR><TD>(5)</TD> <TD>0</TD> <TD>0</TD>	<TD>0</TD>	<TD>0</TD>	<TD>1</TD></TR>
</TABLE>
<H2>7.6. ADS Constrained Optimization Solvers </H2>

<h3>7.6.1. Solver APOLLO</h3>
<P>The following solvers, APOLLO, ATLAS, ARGUS, ODIN, HELIOS, CRONUS, DEMETER, 
and CERES are adaptations of the Automated Design Synthesis (ADS) algorithms 
developed for NASA by G. N. Vanderplaats and coworkers at the Naval Postgraduate 
School in Monterey, California. They have been integrated to conform with FC 
output formats and their control variables have been adapted to conform to the 
precedents of FC. They are available in a separate library as an extension to FC</P>

<A NAME="APOLLO" ID="APOLLO"></A>

<P>APOLLO applies the sequential unconstrained minimization technique, 
the same "grand strategy" as ZEUS and JOVE, with optional variations 
in strategy, optimization method, and vector searching.</P>

<H4>The strategy variations include:</H4> 

<OL TYPE="1">
<LI>The exterior penalty function method:</LI> 
<UL>
<LI>Fox, R. L.: "Optimization Methods for Engineering Design," 
Addison-Wesley, 1971;</LI> 
</UL>
<LI>The linear-extended penalty function method:</LI>
<UL>
<LI>Kavlie, D., and Moe, J.:  "Automated Design of Frame Structures," 
ASCE Journal of Structural Div., Vol. ST1, Jan. 1971, pp. 33-62;</LI> 

<LI>Cassis, J. H.:  "Optimum Design of Structures Subjected 
to Dynamic Loads."  Ph.D. Thesis, University of California, Los Angeles, 
1974;</LI> 

<LI>Cassis, J. H., and Schmit, L. A.:  "On Implementation 
of the Extended Interior Penalty Function."  International Journal 
of Numerical Methods in Engineering, Vol. 10, No. 1, 1976, pp. 3-23;</LI>
</UL>

<LI>The quadratic extended interior penalty function 
method: 
<UL>
<LI>Haftka, R. T., and Starnes, J. H., Jr.:  "Application 
of a Quadratic Extended Interior Penalty Function for Structural Optimization."  AIAA 
Journal, Vol. 14, June 1976, pp. 718-724;</LI> 

<LI>Prasad, B., and Haftda, R. T.:  "Optimum Structural Design 
with Plate Finite Elements."  ASCE Journal of Structural Div., Vol. 
ST11, Nov. 1979, pp. 2367-2382;</LI>
</UL>

<LI>The cubic extended interior penalty function method:</LI> 

<UL>
<LI>Prasad, B.:  "A Class of Generalized Variable Penalty 
Methods for Nonlinear Programming."  Journal of Optimization Theory 
and Applications, Vol. 35, No. 2, Oct. 1981, pp. 159-182.</LI>
</UL>
</OL>

<h4>The optimizer variations include:</H4>  

<OL TYPE = "1">
<LI>The Fletcher-Reeves conjugate gradient algorithm for 
unconstrained minimization:</LI> 
<UL>
<LI>Fletcher, R., and Reeves, C. M.:  "Function Minimization 
by Conjugate Gradients."  Computer J., Vol. 7, No. 2, 1964, pp. 149-154;</LI>
</UL>

<LI>The Davidon-Fletcher-Powell (DFP) variable-metric 
method for unconstrained minimization:</LI> 
<UL>
<LI>Davidon, W. C.:  "Variable Metric Method for Minimization,"  Argone 
National Laboratory, ANL-5990 Rev., University of Chicago, 1959;</LI> 

<LI>Fletcher, R. and Powell, M. J. D.,  "A Rapidly Convergent 
Method for Minimization," Computer Journal, Vol. 6, No. 2, 1963, pp. 
163-168;</LI>
</UL>

<LI>The Broydon-Fletcher-Goldfarb-Shanno (BFGS) variable 
metric method for unconstrained minimization:</LI> 
<UL>
<LI>Broydon, C. G.,  "The Convergence of a Class of Double 
Rank Minimization Algorithms," Parts I and II, J. Inst. Maths. Applns., 
Vol. 6, 1970, pp. 76-90 and 222-231;</LI> 

<LI>Fletcher, R.,  "A New Approach to Variable Metric Algorithms," 
Computer Journal, Vol. 13, 1970, pp. 317-322. Goldfarb, D.,  "A Family 
of Variable Metric Methods Derived by Variational Means,"  Maths. 
Comput., Vol. 24, 1970, pp. 23-36;</LI>

<LI>Shanno, D. F.,  "Conditional of Quasi-Newton Methods for 
Function Minimization,"  Maths. Comput., Vol. 24, 1970, pp. 647-656.</LI>  
</UL>
</OL>
<H4>The variations in vector search methods include:</H4> 
<OL TYPE="1">
<LI>The Golden Section Method,</LI> 

<LI>The Golden Section method followed by polynomial 
interpolation,</LI> 

<LI>Bounded polynomial interpolation,</LI> 

<LI>Unbounded polynomial interpolation.</LI>
</OL>



<A NAME="FIND_APOLLO" ID="FIND_APOLLO"><P>The general form of the FIND statement for APOLLO is</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B>&nbsp;<i>unknowns</i><B>;</B>&nbsp;<B>IN</B>&nbsp;<i>modelcall</i><B>; 
BY APOLLO</B> {</B>(<i>controller</i><B>)</B>}<B>;</B>  <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>} {{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>}{{<B>WITH</B>} <B>FLAG</B> <i>signal</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The <i>modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="APOLLO_CTL" ID="APOLLO_CTL"><B><i>APOLLO Controls</i></B></A> - The control variables for APOLLO are as follows:
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>STRATEGY</B></TD><TD>1</TD><TD>Exterior penalty function method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Linear extended penalty function method 		 </TD><TD></TD></TR>
			

<TR><TD></TD><TD>3</TD><TD>Quadratic extended penalty function method 		</TD><TD></TD></TR>
			

<TR><TD></TD><TD>4</TD><TD>Cubic extended penalty function method</TD><TD></TD></TR>
			 

<TR><TD><B>OPTIMIZR</B></TD><TD>1</TD><TD>Fletcher-Reeves conjugate gradient method </TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Davidon-Fletcher-Powell (DFP) variable-metric 
			method</TD><TD></TD></TR>
			

<TR><TD></TD><TD>3</TD><TD>Broydon-Fletcher-Goldfarb-Shanno 
			variable-metric method </TD><TD></TD></TR>
            	

<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation 	</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete</TD><TD>1</TD></TR>
			
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the current step in 
			which an ABORT statement was executed.</TD><TD></TD></TR>
        	

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last
</TD><TD></TD></TR>
			
<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL </TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>
			 

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction and variable metric methods.  Unconstrained 
			minimization  is restarted with a steepest descent 
			direction every REDO iterations.
</TD><TD>n<SUB>iv</SUB>+1</TD></TR>
			
<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints
			are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the</TD><TD>40</TD></TR>

<TR><TD><B>CONSECO</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or relative convergence criteria 
			must be met to indicate convergence 
			at the optimizer level.
</TD><TD>3</TD></TR>
			
<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>CONSECS</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or relative convergence criteria must
			be met to indicate convergence at the strategy level.</TD><TD>2</TD></TR>
			

<TR><TD><B>CONVERGS</B></TD><TD></TD><TD>Strategy convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
				

<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>ABCONV</B></TD><TD></TD><TD>Absolute convergence criteria for the one-dimensional search when using the 
			Golden Section method. </TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESO</B></TD><TD></TD><TD>Maximum absolute fractional change in the objective between two consecutive iterations 
			to indicate convergence in optimization.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABCONO</B></TD><TD></TD><TD>Absolute fractional convergence criterion for the optimization sub-problem when 
			using sequential minimization techniques.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESS</B></TD><TD></TD><TD>Same as ABGRESO but used at the strategy level.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>RECONV</B></TD><TD></TD><TD>Relative convergence criteria for the one-dimensional search when using the Golden Section method.</TD><TD>0.005</TD></TR>
			

<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between two consecutive iterations to indicate 
			convergence in optimization.
</TD><TD>0.001</TD></TR>
			
<TR><TD><B>RECONO</B></TD><TD></TD><TD>Relative convergence criterion for the optimization sub-problem when using 
			sequential minimization techniques.</TD><TD>0.01</TD></TR>
			

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Same as PROGRESO, but used at the strategy level</TD><TD>0.001</TD></TR>

<TR><TD><B>REDELOI</B></TD><TD></TD><TD>Relative change in the objective function attempted on the first optimization iteration. 
			Used to estimate initial move in the one-imensional search.  Updated as the optimization progresses.</TD><TD>0.1</TD></TR>
			

<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function attempted on the first optimization iteration. 
			Used to estimate initial move in the one-dimensional search.  Updated as the 
			optimization progresses. </TD><TD>1000</TD></TR>
			

<TR><TD><B>REDELXI </B></TD><TD></TD><TD>Maximum relative change in a design variable attempted on the first optimization iteration. Used to estimate 
            the initial move in the one-dimensional search.  Updated as the optimization progresses. </TD><TD>0.01</TD></TR>
			

<TR><TD><B>ABDELXI </B></TD><TD></TD><TD>Maximum absolute change in a design variable attempted on the first optimization iteration. 
            Used to estimate the initial move in the one-dimensional search. 
            Updated as the optimization progresses. 
</TD><TD>0.2</TD></TR>
			
<TR><TD><B>PENEPS </B></TD><TD></TD><TD>Initial transition point for extended penalty function methods. 
Updated as the optimization progresses.</TD><TD>-0.05</TD></TR>
			 

<TR><TD><B>EXTRAP</B></TD><TD></TD><TD>Maximum multiplier on the search step 
in the one-dimensional search using polynomial 
interpolation/extrapolation.</TD><TD>5</TD></TR>
			 

<TR><TD><B>PEPMULT</B></TD><TD></TD><TD>Penalty function multiplier for the 
exterior penalty function method.  Must be greater than 1.0</TD><TD>5</TD></TR>
			

<TR><TD><B>PEP</B></TD><TD></TD><TD>Initial penalty parameter for the exterior 
penalty function method or the Augmented Lagrange Multiplier method.</TD><TD>0</TD></TR>
			

<TR><TD><B>PEPMAX </B></TD><TD></TD><TD>Maximum value of PEP for the exterior 
penalty function method or the Augmented Lagrange Multiplier method.</TD><TD>7E+10</TD></TR>
			 

<TR><TD><B>PEPRED</B></TD><TD></TD><TD>Multiplier on PEP for consecutive iterations.</TD><TD>0.2</TD></TR>

<TR><TD><B>PIPCON</B></TD><TD></TD><TD>Minimum value of PIP to indicate</TD><TD>7E-10</TD></TR>
			

<TR><TD><B>PIP</B></TD><TD></TD><TD>Initial penalty parameter for 
extended interior penalty function methods. </TD><TD>100</TD></TR>
			

<TR><TD><B>SCALO</B></TD><TD></TD><TD>The user-supplied value of the scale factor 
for the objective function if the default or 
calculated value is to be over-ridden.</TD><TD>1.0</TD></TR>
			

<TR><TD><B>SCALMIN</B></TD><TD></TD><TD>Minimum numerical value of any scale factor allowed.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>VMULT</B></TD><TD></TD><TD>Multiplier on the move parameter, ALPHA, 
dimensional search to find bounds on the solution.</TD><TD>2.618034</TD></TR>
			

<TR><TD><B>ZRO</B></TD><TD></TD><TD>Numerical estimate of zero on the computer 
Usually the default value is adequate. If a computer with a short word length is used, 
ZRO=10E-4 may be preferred.</TD><TD>0.00001</TD></TR>
</TABLE>
<HR>			

<h3>7.6.2. Solver ATLAS</h3>

<P>ATLAS applies the Augmented Lagrange Multiplier strategy:</P> 
<UL>
<li>Rockafellar, R. T.:  "The Multiplier Method of Hestenes 
and Powell Applied to Convex Programming."  Journal of Optimization 
Theory and Application, Vo. 12, No. 6, 1973, pp. 555-562;</li> 

<li>Pierre, D. A., and Lowe, M. J.:  "Mathematical Programming 
Via Augmented Lagrangians."  Applied Mathematics and Computation Series, 
Addison-Wesley, 1975;</li>  

<li>Powell, M. J. D.:  "Algorithms for Nonlinear Constraints 
that Use Lagrangian Functions.": Mathematical Programming, Vol. 14, 
No. 2, 1978, pp. 224-248;</li>

<li>Imai, K.:  "Configuration Optimization of Trusses by the 
Multiplier Method."  Ph.D. Thesis, University of California, Los Angeles, 
1978;</li> 

<li>Imai, K.; and Schmit, L. A.:  "Configuration Optimization 
of Trusses."  Journal of the Structural Division, ASCE, Vol. 107, 
No. ST5, May 1981, pp.745-756.</li> 
</ul>

<P>It has the same optimizer and vector search variations of APOLLO.</P> 


<A NAME="FIND_ATLAS" ID="FIND_ATLAS"><P>The general form of the FIND statement for ATLAS is</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>modelcall</i><B>;</B> 
<B>BY ATLAS</B> {<B>(</b><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>}{{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {{<B>WITH</B>} <B>FLAG</B> 
<i>signal</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</b><i> inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>}<BR> 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The <i>modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="ATLAS" ID="ATLAS_CTL"><P><B><i>ATLAS Controls</i></B></A> - The control variables for ATLAS are as follows:</P>
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>OPTIMIZR</B></TD><TD>1</TD><TD>Fletcher-Reeves conjugate gradient method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Davidon-Fletcher-Powell (DFP) variable-metric method</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>3</TD><TD>Broydon-Fletcher-Goldfarb-Shanno variable-metric method</TD><TD></TD></TR>
        	 

<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>
			
<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete</TD><TD>1</TD></TR>
			
            
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the current step in 
			which an ABORT statement was executed.</TD><TD></TD></TR>
        	

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>
			 

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction and minimization 
is restarted with a steepest descent direction every REDO iterations.</TD><TD>n<SUB>iv</SUB>+1</TD></TR>
			

<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints
			are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the</TD><TD>40</TD></TR>

<TR><TD><B>CONSECO</B></TD><TD></TD><TD>The number of consecutive iterations for which 
the absolute and/or relative convergence criteria 
must be met to indicate convergence at the optimizer level.</TD><TD>3</TD></TR>
			

<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>CONSECS</B></TD><TD></TD><TD>The number of consecutive iterations for which 
the absolute and/or relative convergence criteria must be met to indicate convergence at the strategy level.
</TD><TD>2</TD></TR>
			
<TR><TD><B>CONVERGS</B></TD><TD></TD><TD>Strategy convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>ALAM</B></TD><TD></TD><TD>Initial estimate of the Lagrange Multipliers 
in the Augmented Lagrange Multiplier Method. </TD><TD>0.0</TD></TR>
			
<TR><TD><B>ABCONV</B></TD><TD></TD><TD>Absolute convergence criteria for the one-dimensional 
search when using the Golden Section method.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESO</B></TD><TD></TD><TD>Maximum absolute fractional change in the 
objective between two consecutive iterations to indicate convergence in optimization.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABCONO</B></TD><TD></TD><TD>Absolute fractional convergence criterion for the optimization sub-problem when 
			using sequential minimization techniques.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESS</B></TD><TD></TD><TD>Same as ABGRESO but used at the strategy level.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>RECONV</B></TD><TD></TD><TD>Relative convergence criteria for the one-dimensional search 
when using the Golden Section method.</TD><TD>0.005</TD></TR>
			
            
<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between two 
consecutive iterations to indicate convergence in optimization.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>RECONO</B></TD><TD></TD><TD>Relative convergence criterion for the 
optimization sub-problem when using sequential minimization techniques.
</TD><TD>0.01</TD></TR>
			
<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Same as PROGRESO, but used at the strategy level.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>REDELOI</B></TD><TD></TD><TD>Relative change in the objective 
function attempted on the first optimization iteration. 
            Used to estimate initial move in the one-dimensional search.  
            Updated as the optimization progresses.
</TD><TD>0.1</TD></TR>
			
<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function attempted on the first optimization iteration. 
            Used to estimate initial move in the one-dimensional search. 
            Updated as the optimization progresses. 
</TD><TD>1000</TD></TR>
			
<TR><TD><B>REDELXI </B></TD><TD></TD><TD>Maximum relative change in a design variable attempted on the first optimization iteration. 
            Used to estimate the initial move in the one-dimensional search. 
            Updated as the optimization progresses. 
</TD><TD>0.01</TD></TR>
			
<TR><TD><B>ABDELXI </B></TD><TD></TD><TD>Maximum absolute change in a design variable attempted on the first optimization iteration. 
            Used to estimate the initial move in the one-dimensional 
            search. Updated as the optimization progresses. </TD><TD>0.2</TD></TR>
			

<TR><TD><B>PEPMULT</B></TD><TD></TD><TD>Penalty function multiplier for the 
exterior penalty function method.  Must be greater than 1.0.</TD><TD>5</TD></TR>
			
<TR><TD><B>PEP</B></TD><TD></TD><TD>Initial penalty parameter for the exterior 
penalty function method or the Augmented Lagrange Multiplier method.</TD><TD>0</TD></TR>
			

<TR><TD><B>PEPMAX </B></TD><TD></TD><TD>Maximum value of PEP for the exterior penalty 
function method or the Augmented Lagrange Multiplier method. 
</TD><TD>7E+10</TD></TR>
			 
<TR><TD><B>SCALMIN</B></TD><TD></TD><TD>Minimum numerical value of any scale factor allowed.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>VMULT</B></TD><TD></TD><TD>Multiplier on the move parameter, ALPHA, 
in the one-dimensional search to find bounds on the solution.</TD><TD>2.618034</TD></TR>
			

<TR><TD><B>ZRO</B></TD><TD></TD><TD>Numerical estimate of zero on the computer Usually the default value is adequate. 
            If a computer with a short word length is 
            used, ZRO=10E-4 may be preferred.
</TD><TD>0.00001</TD></TR>
</TABLE>
<HR>
			
<h3>7.6.3. Solver ODIN</h3>

<P>ODIN applies the sequential Linear Programming (SLP) strategy:</P> 

<ul>
<li>Kelley, J. E.: "The Cutting Plane Method for Solving Convex 
Programs." J. SIAM, 1960, pp. 703-712;</li> 

<li>Moses, F.: "Optimum Structural Design Using Linear Programming." 
Proc. ASCE, Vol. 90, No. ST6, 1964, pp. 89-104.</li>
</ul>

<P>It provides optional selection of two optimizers and four vector search 
algorithms. The optimizer options include:</P> 
<ol type="1">
<li>The Method of Feasible Directions (MFD):</li> 
<ul>
<li>Zoutendijk, M., "Methods of Feasible Directions," Elsevier 
Publishing Co., Amsterdam, 1960;</li> 

<li>Vanderplaats, G. N. and Moses, F., "Structural Optimization 
by Methods of Feasible Directions," Journal of Computers and Structures, 
Vol. 3, Pergamon Press, July 1973, pp. 739-755;</li>
</ul>
<li>The Modified Method of Feasible Directions (MMFD):</li> 
<ul>

<li>Vanderplaats, G. N., "A Robust Feasible Directions Algorithm 
for Design Synthesis," Proc. AIAA/ASME/ASCE/AHS 24th Structures, Structural 
Dynamics and Materials Conference, Lake Tahoe, Nevada, May 2-4, 1983.</li>
</ul>
</ol>

<P>The variations in vector search methods are the same as those of APOLLO.</P>

<A NAME="FIND_ODIN" ID="FIND_ODIN"><P>The general form of the FIND statement for ODIN is:</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>modelcall</i><B>; BY 
ODIN </B>{<B>(</b><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>}<B> LOWER</B> <i>floor</i><B>;</B>} {{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i>;} {{<B>WITH</B>} <B>FLAG</B> 
<i>signal</i>;} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i>;} {<B>MATCHING</B> <i>equalities</i>;}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The <i>modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="ODIN_CTL" ID="ODIN_CTL"><P><B><i>ODIN Controls</i></B></A> - The control variables for ODIN are as follows:</P>
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>OPTIMIZR</B></TD><TD>1</TD><TD>Method of Feasible Directions (MFD)</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Modified Method of Feasible Directions (MMFD)X</TD><TD></TD></TR>
			

<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete.</TD><TD>1</TD></TR>
			
            
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the current step in	which an ABORT statement was executed.</TD><TD></TD></TR>
        	

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>
<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT </B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>
			 

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction and 
variable metric methods. Unconstrained minimization is restarted with a steepest descent direction every REDO iterations.
</TD><TD>n<SUB>iv</SUB>+1</TD></TR>
			
<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the optimizer level. </TD><TD>40</TD></TR>
			

<TR><TD><B>CONSECO</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or relative convergence 
criteria must be met to indicate convergence at the optimizer level.</TD><TD>3</TD></TR>
			

<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>CONSECS</B></TD><TD></TD><TD>The number of consecutive iterations for which the 
absolute and/or relative convergence criteria must be 
met to indicate convergence at the strategy level.</TD><TD>2</TD></TR>
			
<TR><TD><B>CONVERGS</B></TD><TD></TD><TD>Strategy convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>CTOL</B></TD><TD></TD><TD>Constraint tolerance in the Method of Feasible Directions or the Modified Method of Feasible Directions. 
            A constraint is active if its numerical value is more positive than CTOL </TD><TD>-0.03</TD></TR>
			

<TR><TD><B>CLTOL</B></TD><TD></TD><TD>Same as CTOL but for linear constraints.</TD><TD>-0.005</TD></TR>

<TR><TD><B>CLTOLMIN</B></TD><TD></TD><TD>Same as CTOLMIN, but for linear constraints.</TD><TD>0.001</TD></TR>

<TR><TD><B>CTOLMIN</B></TD><TD></TD><TD>Minimum constraint tolerance for nonlinear constraints. 
If a constraint is more positive than CTOLMIN, it is considered to be violated. </TD><TD>0.01</TD></TR>
			

<TR><TD><B>RIMOVE </B></TD><TD></TD><TD>Initial relative move limit. Used to set the move 
limits in sequential linear programming, method of inscribed 
            hyperspheres and sequential quadratic programming 
            as a fraction of the value of the unknowns.. </TD><TD>0.2</TD></TR>
			

<TR><TD><B>ABCONV</B></TD><TD></TD><TD>Absolute convergence criteria for the 
one-dimensional search when using the Golden Section method.
</TD><TD>0.0001</TD></TR>
			
<TR><TD><B>ABGRESO</B></TD><TD></TD><TD>Maximum absolute fractional change in the objective 
between two consecutive iterations to indicate convergence in optimization.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABCONO</B></TD><TD></TD><TD>Absolute fractional convergence criterion for the 
optimization sub-problem when using sequential minimization techniques</TD><TD>0.0001</TD></TR

<TR><TD><B>ABGRESS</B></TD><TD></TD><TD>Same as ABGRESO but used at the strategy level.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>RECONV</B></TD><TD></TD><TD>Relative convergence criteria for the one-dimensional 
search when using the Golden Section method.</TD><TD>0.005</TD></TR>
			
<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between 
two consecutive iterations to indicate convergence in optimization.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>RECONO</B></TD><TD></TD><TD>Relative convergence criterion for the optimization 
sub-problem when using sequential minimization techniques.</TD><TD>0.01</TD></TR>
			
<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Same as PROGRESO, but used at the strategy level.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>REDELOI</B></TD><TD></TD><TD>Relative change in the objective function attempted on the first 
optimization iteration. Used to estimate initial move in the one-dimensional search. 
Updated as the optimization progresses.</TD><TD>0.1</TD></TR>
			

<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function attempted 
on the first optimization iteration. Used to estimate initial move in the one-dimensional 
search. Updated as the optimization progresses. 
</TD><TD>1000</TD></TR>
			
<TR><TD><B>REDELXI </B></TD><TD></TD><TD>Maximum relative change in a design variable attempted 
on the first optimization iteration. Used to estimate the initial move 
in the one-dimensional search.  Updated as the optimization progresses. 
</TD><TD>001</TD></TR>
			
	<tr>
		
	</tr>
</TABLE>
<h3>7.6.4. Solver HELIOS</h3>

<P>HELIOS applies the Method of Centers or Method of Inscribed Hyperspheres:</P> 

<UL>
<LI>Baldur, R.: "Structural Optimization by Inscribed Hyperspheres." 
Journal of Engineering Mechanics, ASCE, Vol. 98, No. EM3, June 1972, 
pp. 503-508.</LI> 
</UL>

<P>It provides optional selection of two optimizers and four vector search 
algorithms.  The optimizer options are the same as for ODIN, and the 
vector search options are the same as for APOLLO.</P>

<A NAME="FIND_HELIOS" ID="FIND_HELIOS"><P>The general form of the FIND statement for HELIOS is:</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>modelcall</i><B>; BY 
HELIOS </B>{<B>(</B><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>} {{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {{<B>WITH</B>} <B>FLAG</B> 
<i>signal</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The<i> modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="HELIOS_CTL" ID="HELIOS_CTL"><P><B><i>HELIOS Controls</i></B></A> - The control variables of HELIOS are as follows:</P>
<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>OPTIMIZR</B></TD><TD>1</TD><TD>Method of Feasible Directions (MFD) </TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Modified Method of Feasible Directions (MMFD)</TD><TD></TD></TR>
		 	 

<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete.</TD><TD>1</TD></TR>
			
            
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the current step in which an ABORT statement was executed.</TD><TD></TD></TR>
            	

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>
			

<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL</TD><TD></TD></TR>
			

<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>
			 

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction and variable metric methods.  
Unconstrained minimization is restarted with a steepest descent direction every REDO iterations.</TD><TD>n<SUB>iv</SUB>+1</TD></TR>
			

<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints	are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the optimizer level.</TD><TD>40</TD></TR>
			  

<TR><TD><B>CONSECO</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or relative convergence 
criteria must be met to indicate convergence at the optimizer level.</TD><TD>3</TD></TR>
			

<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>CONSECS</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or 
relative convergence criteria must be met to indicate convergence at the strategy level.</TD><TD>2</TD></TR>
			

<TR><TD><B>CONVERGS</B></TD><TD></TD><TD>Strategy convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>ABGRESO</B></TD><TD></TD><TD>Maximum absolute fractional change in the objective 
between two consecutive iterations to indicate convergence in optimization.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABCONO</B></TD><TD></TD><TD>Absolute fractional convergence criterion for the optimization 
sub-problem when using sequential minimization techniques.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESS</B></TD><TD></TD><TD>Same as ABGRESO but used at the strategy level.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>RECONV</B></TD><TD></TD><TD>Relative convergence criteria for the 
one-dimensional search when using the Golden Section method. </TD><TD>0.005</TD></TR>
			

<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between two 
consecutive iterations to indicate convergence in optimization.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>RECONO</B></TD><TD></TD><TD>Relative convergence criterion for the 
optimization sub-problem when using sequential minimization techniques.</TD><TD>0.01</TD></TR>
			

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Same as PROGRESO, but used at the strategy level.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>REDELOI </B></TD><TD></TD><TD>Relative change in the objective function attempted on the first optimization iteration. 
Used to estimate initial move in the one-dimensional search. Updated as the optimization progresses.</TD><TD>0.1</TD></TR>
			

<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function attempted on the first optimization 
iteration. Used to estimate initial move in the one-dimensional search. Updated as the optimization progresses. </TD><TD>1000</TD></TR>
			

<TR><TD><B>REDELXI</B></TD><TD></TD><TD>Maximum relative change in a design variable attempted 
on the first optimization iteration. Used to estimate the initial move in the one-dimensional 
search. Updated as the optimization progresses. 
</TD><TD>0.01</TD></TR>
			
<TR><TD><B>ABDELXI</B></TD><TD></TD><TD>Maximum absolute change in a design variable attempted on the first optimization iteration. 
Used to estimate the initial move in the one-dimensional search. Updated as the optimization progresses. </TD><TD>0.2</TD></TR>
			

<TR><TD><B>EXTRAP</B></TD><TD></TD><TD>Maximum multiplier of the search step in the 
one-dimensional search using polynomial interpolation/extrapolation. </TD><TD>5</TD></TR>
			

<TR><TD><B>STEEPD</B></TD><TD></TD><TD>Additional steepest descent fraction in the 
method of centers. After moving to the center of the hypersphere, a steepest descent 
move is made equal to STEEPD times the radius of the hypersphere. </TD><TD>0.0</TD></TR>
			

<TR><TD><B>CTOL </B></TD><TD></TD><TD>Same as CTOL but for linear constraints. 
Feasible Directions or the Modified Method of Feasible Directions. A constraint 
is active if its numerical value is more positive than CTOL 
</TD><TD>-0.03</TD></TR>
			
<TR><TD><B>CLTOL</B></TD><TD></TD><TD>Constraint tolerance in the Method of</TD><TD>-0.005</TD></TR>

<TR><TD><B>CLTOLMIN </B></TD><TD></TD><TD>Same as CTOLMIN, but for linear constraints.</TD><TD>0.001</TD></TR>

<TR><TD><B>CTOLMIN</B></TD><TD></TD><TD>Minimum constraint tolerance for nonlinear constraints. 
If a constraint is more positive than CTOLMIN, it is considered to be violated. </TD><TD>0.01</TD></TR>
			

<TR><TD><B>RIMOVE </B></TD><TD></TD><TD>Initial relative move limit. Used to set the move limits in sequential 
linear programming, method of inscribed hyperspheres and sequential quadratic programming 
as a fraction of the value of the unknowns. 
</TD><TD>0.2</TD></TR>
			
<TR><TD><B>SCALO</B></TD><TD></TD><TD>The user-supplied value of the scale factor for the objective 
function if the default or calculated value is to be over-ridden.</TD><TD>1.0</TD></TR>
			

<TR><TD><B>SCALMIN</B></TD><TD></TD><TD>Minimum numerical value of any scale factor allowed.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>STOL </B></TD><TD></TD><TD>Tolerance on the components of the calculated search 
direction to indicate that the Kuhn-Tucker conditions are satisfied. </TD><TD>0.001</TD></TR>
			

<TR><TD><B>PUSHOFF</B></TD><TD></TD><TD>Nominal value of the push-off 
factor in the Method of Feasible Directions.</TD><TD>0.1</TD></TR>
			 

<TR><TD><B>VMULT</B></TD><TD></TD><TD>Multiplier of the search step 
in the one-dimensional search to find bounds on the solution.</TD><TD>2.618034</TD></TR>
			

<TR><TD><B>ZRO</B></TD><TD></TD><TD>Numerical estimate of zero on the computer Usually the default value is adequate. 
If a computer with a short word length is used, ZRO=10E-4 may be preferred.</TD><TD>0.00001</TD></TR>
</TABLE>
<HR>			

<h3>7.6.5. Solver CRONUS</h3>

<P>CRONUS applies the method of Sequential Quadratic Programming (SQP):</P> 

<UL>
<LI>Powell, M. J. D.: "An Efficient Method for Finding the Minimum 
of a Function of Several Variables without Calculating Derivatives." 
Computer J., 7 (4), 1964, pp. 303-307;</LI>

<LI>Powell, M. J. D.: "A Fast Algorithm for Nonlinearly Constrained 
Optimization Calculations." Report DAMTP77/NA2, University of Cambridge, 
England, 1977.</LI>
</UL>

<P>It provides optional selection of two optimizers and four vector search 
algorithms. The optimizer options are the same as for ODIN, and the 
vector search options are the same as for APOLLO.</P>

<A NAME="FIND_CRONUS" ID="FIND_CRONUS"><P>The general form of the FIND statement for CRONUS is:</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>modelcall</i><B>;  BY 
CRONUS</B>{<B>(</B><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>} {{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {{<B>WITH</B>} <B>FLAG</B><i>signal</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The <i>modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="CRONUS_CTL" ID="CRONUS_CTL"><P><B><i>CRONUS Controls</i></B></A> - The control variables of CRONUS are as follows:</P>

<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>OPTIMIZR</B></TD><TD>1</TD><TD>Method of Feasible Directions (MFD) </TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Modified Method of Feasible Directions (MMFD) </TD><TD></TD></TR>
		 	

<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation</TD><TD></TD></TR>
			 	        
<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>        
			

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete.</TD><TD>1</TD></TR>
			
            
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the current step in which an ABORT statement was executed.</TD><TD></TD></TR>            
			

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>        
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL </TD><TD></TD></TR>
        
<TR><TD></TD><TD>-1</TD><TD>Summary report to Console </TD><TD></TD></TR>        
			

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction and variable metric methods. 
Unconstrained minimization is restarted with a steepest descent direction every REDO iterations.</TD><TD>n<MV>iv</B>+1</TD></TR>
			

<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints	are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the optimizer level. </TD><TD>40</TD></TR>
			 

<TR><TD><B>CONSECO</B></TD><TD></TD><TD>The number of consecutive iterations for which 
the absolute and/or relative convergence criteria must be met to indicate convergence at the optimizer level.
</TD><TD>3</TD></TR>
			
<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>        
			

<TR><TD><B>CONSECS</B></TD><TD></TD><TD>The number of consecutive iterations for which the 
absolute and/or relative convergence criteria must be met to indicate 
convergence at the strategy level.</TD><TD>2</TD></TR>
			

<TR><TD><B>CONVERGS</B></TD><TD></TD><TD>Strategy convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>        
			

<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>SVSEARCH</B></TD><TD></TD><TD>The one-dimensional search method to be used in the 
Sequential Quadratic Programming method at the strategy level. </TD><TD>1</TD></TR>
			
            
<TR><TD></TD><TD>1</TD><TD>Golden-section search method </TD><TD></TD></TR>
        
<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
        
<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation </TD><TD></TD></TR>        
        
<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>        
		 	 

<TR><TD><B>ABCONV</B></TD><TD></TD><TD>Absolute convergence criteria for the 
one-dimensional search when using the Golden Section method. </TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESO</B></TD><TD></TD><TD>Maximum absolute fractional change in the objective 
between two consecutive iterations to indicate convergence in optimization.
</TD><TD>0.0001</TD></TR>
			
<TR><TD><B>ABCONO</B></TD><TD></TD><TD>Absolute fractional convergence criterion
for the optimization sub-problem when using sequential minimization techniques.
</TD><TD>0.0001</TD></TR>
			
<TR><TD><B>ABGRESS</B></TD><TD></TD><TD>Same as ABGRESO but used at the strategy level.</TD><TD>0.0001</TD></TR>

<TR><TD><B>RECONV</B></TD><TD></TD><TD>Relative convergence criteria for the 
one-dimensional search when using the Golden Section method.</TD><TD>0.005</TD></TR>
			

<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between 
two consecutive iterations to indicate convergence in optimization.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>RECONO</B></TD><TD></TD><TD>Relative convergence criterion for the 
optimization sub-problem when using sequential minimization techniques.</TD><TD>0.01</TD></TR>
			

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Same as PROGRESO, but used at the strategy level.</TD><TD>0.001</TD></TR>

<TR><TD><B>REDELOI</B></TD><TD></TD><TD>Relative change in the objective function
 attempted on the first optimization iteration. Used to estimate initial move in the one-dimensional search. 
            Updated as the optimization progresses.
</TD><TD>0.1</TD></TR>
			
<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function 
attempted on the first optimization iteration. Used to estimate initial move 
in the one-dimensional search.  Updated as the optimization progresses. 
</TD><TD>1000</TD></TR>
			
<TR><TD><B>REDELXI </B></TD><TD></TD><TD>Maximum relative change in a design 
variable attempted on the first optimization iteration. Used to estimate the 
            initial move in the one-dimensional search.  
            Updated as the optimization progresses. </TD><TD>0.01</TD></TR>
			

<TR><TD><B>ABDELXI </B></TD><TD></TD><TD>Maximum absolute change in a design variable 
attempted on the first optimization iteration. Used to 
            estimate the initial move in the one-dimensional search. 
            Updated as the optimization progresses. </TD><TD>0.2</TD></TR>
			

<TR><TD><B>EXTRAP</B></TD><TD></TD><TD>Maximum multiplier of the search step 
in the one-dimensional search using polynomial interpolation/extrapolation. </TD><TD>5</TD></TR>
			

<TR><TD><B>SCALO</B></TD><TD></TD><TD>The user-supplied value of the scale factor for the 
objective function if the default or calculated value is to be over-ridden.</TD><TD>1.0</TD></TR>
			

<TR><TD><B>SCALMIN</B></TD><TD></TD><TD>Minimum numerical value of any scale factor allowed.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>CTOL </B></TD><TD></TD><TD>Constraint tolerance in the Method of 
Feasible Directions or the Modified Method of Feasible Directions. 
A constraint is active if its numerical value is more positive than CTOL 
</TD><TD>-0.03</TD></TR>
			
<TR><TD><B>CLTOL</B></TD><TD></TD><TD>Same as CTOL but for linear constraints.</TD><TD>-0.005</TD></TR>

<TR><TD><B>CLTOLMIN </B></TD><TD></TD><TD>Same as CTOLMIN, but for linear constraints.</TD><TD>0.001</TD></TR>

<TR><TD><B>CTOLMIN </B></TD><TD></TD><TD>Minimum constraint tolerance for nonlinear constraints. 
If a constraint is more positive than CTOLMIN, it is considered to be violated. </TD><TD>0.01</TD></TR>
			

<TR><TD><B>RIMOVE </B></TD><TD></TD><TD>Initial relative move limit. Used to set the move limits in 
sequential linear programming, method of inscribed hyperspheres and 
sequential quadratic programming as a fraction of the value of the unknowns. 
</TD><TD>0.2</TD></TR>
			
<TR><TD><B>SQPIP</B></TD><TD></TD><TD>Initial penalty parameter in Sequential Quadratic programming. </TD><TD>10.0</TD></TR>
			
<TR><TD><B>FRACMOV</B></TD><TD></TD><TD>Move fraction to avoid constraint violations 
in Sequential Quadratic Programming. </TD><TD>0.95</TD></TR>
			

<TR><TD><B>STOL</B></TD><TD></TD><TD>Tolerance on the components of the calculated 
search direction to indicate that the Kuhn-Tucker conditions are satisfied.</TD><TD>0.001</TD></TR>
			 

<TR><TD><B>PUSHOFF</B></TD><TD></TD><TD>Nominal value of the push-off 
factor in the Method of Feasible Directions.</TD><TD>0.1</TD></TR>
			 

<TR><TD><B>VMULT</B></TD><TD></TD><TD>Multiplier of the search step in the 
one-dimensional search to find bounds on the solution.</TD><TD>2.618034</TD></TR>
			

<TR><TD><B>ZRO</B></TD><TD></TD><TD>Numerical estimate of zero on the computer. Usually the default value is adequate. 
If a computer with a short word length is used, ZRO=10E-4 may be preferred.</TD><TD>0.00001</TD></TR>
</TABLE>
<HR>			

<h3>7.6.6. Solver DEMETER and CERES</h3>

<P>DEMETER and CERES are constrained optimizers that are normally applied 
under strategic control of ODIN, HELIOS, or CRONUS, but may be applied 
to constrained optimization problems directly without higher strategies. 
DEMETER applies the Method of Feasible Directions (MFD):</P> 

<UL>
<LI>Zoutendijk, M., "Methods of Feasible Directions," Elsevier 
Publishing Co., Amsterdam, 1960;</LI> 

<LI>Vanderplaats, G. N. and Moses, F., "Structural Optimization 
by Methods of Feasible Directions," Journal of Computers and Structures, 
Vol. 3, Pergamon Press, July 1973, pp. 739-755.</LI>
</UL>

<P>CERES applies the Modified Method of Feasible Directions (MMFD):</P> 
<UL>
<LI>Vanderplaats, G. N., "A Robust Feasible Directions Algorithm 
for Design Synthesis," Proc. AIAA/ASME/ASCE/AHS 24th Structures, Structural 
Dynamics and Materials Conference, Lake Tahoe, Nevada, May 2-4, 1983.</LI>
</UL>
<P>The variations in vector search methods are the same as for APOLLO.</P>

<A NAME="FIND_DEMECER" ID="FIND_DEMECER"><P>The general form of the FIND statement for DEMETER and CERES is</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>;</B> <B>IN</B> <i>modelcall</i><B>;   BY 
DEMETER</B>|<B>CERES</B>{<B>(</b><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>} {{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {{<B>WITH</B>} <B>FLAG</b> 
<i>signal</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</b> <i>inequalities</i><B>;</B>} {<B>MATCHING</b> <i>equalities</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The <i>modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="DEMECER_CTL" ID="DEMECER_CTL"><P><B><I>DEMETER and CERES Controls</i></B></A> - The control variables for DEMETER 
and CERES are identical, they are as follows:</P>

<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>

<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation 	</TD><TD></TD></TR>

<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete.</TD><TD>1</TD></TR>
			
            
<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the currentstep in 
which an ABORT statement was executed.</TD><TD></TD></TR>            
			

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>
        
<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>        
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL</TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Summary report to Console</TD><TD></TD></TR>        
			 

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction and variable metric methods. 
Unconstrained minimization is restarted with a steepest descent 
direction every REDO iterations.</TD><TD>n<SUB>iv</SUB>+1</TD></TR>
			

<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the optimizer level.  </TD><TD>40</TD></TR>
			

<TR><TD><B>CONSECO </B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or 
relative convergence criteria must be met to indicate convergence at the optimizer level.</TD><TD>3</TD></TR>
			

<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			
        
<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>        
			
<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>CTOL </B></TD><TD></TD><TD>Constraint tolerance in the Method of 
Feasible Directions or the Modified Method of Feasible Directions. A constraint 
is active if its numerical value is more positive than CTOL 
</TD><TD>-0.03</TD></TR>
			
<TR><TD><B>CLTOL</B></TD><TD></TD><TD>Same as CTOL but for linear constraints.</TD><TD>-0.005</TD></TR>

<TR><TD><B>CLTOLMIN</B></TD><TD></TD><TD>Same as CTOLMIN, but for linear constraints.</TD><TD>0.001</TD></TR>

<TR><TD><B>CTOLMIN</B></TD><TD></TD><TD>Minimum constraint tolerance for nonlinear constraints. 
If a constraint is more positive than CTOLMIN, it is considered to be violated. </TD><TD>0.01</TD></TR>
			

<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between 
two consecutive iterations to indicate convergence in optimization.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>REDELOI</B></TD><TD></TD><TD>Relative change in the objective function a
ttempted on the first optimization iteration. Used to estimate initial move in the one-dimensional search. 
            Updated as the optimization	progresses.
</TD><TD>0.1</TD></TR>

<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function attempted on the first optimization iteration. Used to 
            estimate initial move in the one-dimensional search. Updated as the optimization progresses.</TD><TD>1000</TD></TR>
			 

<TR><TD><B>REDELXI</B></TD><TD></TD><TD>Maximum relative change in a design variable 
attempted on the first optimization iteration. Used to estimate the initial move in 
the one-dimensional search.  Updated as the optimization progresses. 
</TD><TD>0.01</TD></TR>
			
<TR><TD><B>ABDELXI</B></TD><TD></TD><TD>Maximum absolute change in a design variable 
attempted on the first optimization iteration. Used to estimate the initial move in 
the one-dimensional search. Updated as the optimization progresses. 
</TD><TD>0.2</TD></TR>
			
<TR><TD><B>EXTRAP</B></TD><TD></TD><TD>Maximum multiplier of the search step in the 
one-dimensional search using polynomial interpolation/extrapolation. </TD><TD>5</TD></TR>
			

<TR><TD><B>SCALO</B></TD><TD></TD><TD>The user-supplied value of the scale factor for the 
objective function if the default or calculated value is to be over-ridden.</TD><TD>1.0</TD></TR>
			

<TR><TD><B>SCALMIN</B></TD><TD></TD><TD>Minimum numerical value of any scale factor allowed.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>STOL</B></TD><TD></TD><TD>Tolerance on the components of the calculated 
search direction to indicate that the Kuhn-Tucker conditions are satisfied. </TD><TD>0.001</TD></TR>
			

<TR><TD><B>PUSHOFF </B></TD><TD></TD><TD>Nominal value of the push-off 
factor in the Method of Feasible Directions. </TD><TD>0.1</TD></TR>
			

<TR><TD><B>VMULT</B></TD><TD></TD><TD>Multiplier on the search step in the one-dimensional 
search to find bounds on the solution.</TD><TD>2.618034</TD></TR>
			

<TR><TD><B>ZRO</B></TD><TD></TD><TD>Numerical estimate of zero on the computer 
Usually the default value is adequate. If a computer with a short word length is used, ZRO=10E-4 may be preferred.
</TD><TD>0.00001</TD></TR>
</TABLE>
<HR>
			
<A NAME="ARGUS" ID="ARGUS"><H2>Solver ARGUS</H2></A>

<P>ARGUS applies the method of Sequential Convex Programming (SCP):</P> 
<UL>
<LI>Fleury, C. & Braibant, V.: "Structural Optimization, A New 
Dual Method Using Mixed Variables", Int'l Journal for Numerical Methods 
in Engineering, Vol 23, pp. 409-428, 1986</LI>
</UL> 

<P>It provides optional selection of two optimizers and four vector search 
algorithms. The optimizer options are the same as for ODIN, and the 
vector search options are the same as for APOLLO.</P>

<A NAME="FIND_ARGUS" ID="FIND_ARGUS"><P>The general form of the FIND statement for ARGUS is:</P></A>

&nbsp;&nbsp;&nbsp;&nbsp;<B>FIND</B> <i>unknowns</i><B>; IN</B> <i>modelcall</i><B>;  BY  
ARGUS </B>{<B>(</B><i>controller</i><B>)</B>}<B>;</B><BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{{<B>WITH</B>|<B>AND</B>} <B>LOWER</B> <i>floor</i><B>;</B>} {{<B>WITH</B>|<B>AND</B>} 
<B>UPPER</B> <i>ceiling</i><B>;</B>}<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>REPORTING</B> <i>auxiliaries</i><B>;</B>} {{<B>WITH</B>} <B>FLAG</B><i> 
signal</i><B>;</B>} <BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<B>HOLDING</B> <i>inequalities</i><B>;</B>} {<B>MATCHING</B> <i>equalities</i><B>;</B>} 
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>TO MINIMIZE</B>|<B>MAXIMIZE</B> <i>objective</i><BR>

<P>The <i>modelcall</i> symbol is the model name optionally followed 
by an argument list as in the right-hand-side of a CALL statement.</P>

<A NAME="ARGUS_CTL" ID="ARGUS_CTL"><P><B><i>ARGUS Controls</i></B></A> - The control variables of ARGUS are as follows:</P>

<HR>
<TABLE>
<TR><TH align=left>Variable</TH><TH align=left>Value</TH><TH align=center>Option</TH><TH align=left>Preset Value</TH></TR>
<COLGROUP><COL align=left><COL align=center><COL align=left><COL align=right>

<TR><TD><B>OPTIMIZR</B></TD><TD>1</TD><TD>Method of Feasible Directions (MFD) </TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Modified Method of Feasible Directions (MMFD) </TD><TD></TD></TR>
		 	
<TR><TD><B>VSEARCH</B></TD><TD>1</TD><TD>Golden-section search method</TD><TD>1</TD></TR>

<TR><TD></TD><TD>2</TD><TD>Golden-section search+polynomial interpolation</TD><TD></TD></TR>
        
<TR><TD></TD><TD>3</TD><TD>Bounded polynomial interpolation</TD><TD></TD></TR>        
        
<TR><TD></TD><TD>4</TD><TD>Unbounded polynomial interpolation</TD><TD></TD></TR>
			

<TR><TD><B>ABORT</B></TD><TD>0</TD><TD>Delay a model abort until the current iteration is complete</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Execute a model abort after the current step in which an ABORT statement was executed.</TD><TD></TD></TR>
			

<TR><TD><B>DETAIL</B></TD><TD>0</TD><TD>No detailed iteration point</TD><TD>1</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Detailed print every nth iteration plus first and last</TD><TD></TD></TR>
			

<TR><TD><B>SUMMARY</B></TD><TD>1</TD><TD>Print iteration summary</TD><TD>1</TD></TR>

<TR><TD></TD><TD>0</TD><TD>No iteration summary</TD><TD></TD></TR>
			

<TR><TD><B>BREAKIN</B></TD><TD>0</TD><TD>No interactive breakpoints</TD><TD>0</TD></TR>

<TR><TD></TD><TD>n</TD><TD>Breakpoint after each nth iteration</TD><TD></TD></TR>
			

<TR><TD><B>DETOUT</B></TD><TD>0</TD><TD>Detailed report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Detailed report to SCROLL</TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Detailed report to Console</TD><TD></TD></TR>
			

<TR><TD><B>SUMOUT</B></TD><TD>0</TD><TD>Summary report to PRINTER</TD><TD>+1</TD></TR>

<TR><TD></TD><TD>+1</TD><TD>Summary report to SCROLL </TD><TD></TD></TR>

<TR><TD></TD><TD>-1</TD><TD>Summary report to Console </TD><TD></TD></TR>
			

<TR><TD><B>REDO</B></TD><TD></TD><TD>Restart parameter for conjugate direction 
and variable metric methods. Unconstrained minimization is restarted with a 
steepest descent direction every REDO iterations.</TD><TD>n<SUB>iv</SUB>+1</TD></TR>
			

<TR><TD><B>SCALE</B></TD><TD>0</TD><TD>No scaling is done. </TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Design variables, objective and constraints
			are scaled automatically.</TD><TD></TD></TR>
		 	

<TR><TD><B>REMAXO</B></TD><TD></TD><TD>Maximum number of iterations allowed at the optimizer level.</TD><TD>40</TD></TR>
			  

<TR><TD><B>CONSECO</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute and/or 
relative convergence criteria must be met to indicate convergence at the optimizer level.</TD><TD>3</TD></TR>
			

<TR><TD><B>CONVERGO</B></TD><TD></TD><TD>Optimizer convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD><B>CONSECS</B></TD><TD></TD><TD>The number of consecutive iterations for which the absolute 
and/or relative convergence criteria must be met to indicate convergence at the strategy level.</TD><TD>2</TD></TR>
			

<TR><TD><B>CONVERGS</B></TD><TD></TD><TD>Strategy convegence criterion</TD><TD>1</TD></TR>

<TR><TD></TD><TD>1</TD><TD>Satisfy relative <B>or </B>absolute convergence</TD><TD></TD></TR>
			

<TR><TD></TD><TD>2</TD><TD>Satisfy relative <B>and </B>absolute convergence</TD><TD></TD></TR>
			
<TR><TD><B>REMAXS</B></TD><TD></TD><TD>Maximum number of iterations allowed at the strategy level.</TD><TD>20</TD></TR>
			

<TR><TD><B>ABCONV</B></TD><TD></TD><TD>Absolute convergence criteria for the one-dimensional 
search when using the Golden Section method.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESO</B></TD><TD></TD><TD>Absolute fractional convergence criterion objective 
between two consecutive iterations to indicate convergence in optimization.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABCONO</B></TD><TD></TD><TD>Absolute fractional convergence criterion for the 
optimization sub-problem when using sequential minimization techniques.</TD><TD>0.0001</TD></TR>
			

<TR><TD><B>ABGRESS</B></TD><TD></TD><TD>Same as ABGRESO but used at the strategy level.</TD><TD>0.0001</TD></TR>

<TR><TD><B>RECONV</B></TD><TD></TD><TD>Relative convergence criteria for the one-dimensional 
search when using the Golden Section method. </TD><TD>0.005</TD></TR>
			

<TR><TD><B>PROGRESO</B></TD><TD></TD><TD>Maximum relative change in the objective between 
two consecutive iterations to indicate convergence in optimization.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>RECONO</B></TD><TD></TD><TD>Relative convergence criterion for the 
optimization sub-problem when using sequential minimization techniques.</TD><TD>0.01</TD></TR>
			

<TR><TD><B>PROGRESS</B></TD><TD></TD><TD>Same as PROGRESO, but used at the strategy level.</TD><TD>0.001</TD></TR>

<TR><TD><B>REDELOI</B></TD><TD></TD><TD>Relative change in the objective function 
attempted on the first optimization iteration. Used to estimate initial move in the one-dimensional search.  
Updated as the optimization progresses.
</TD><TD>0.1</TD></TR>
			
<TR><TD><B>ABDELOI</B></TD><TD></TD><TD>Absolute change in the objective function 
attempted on the first optimization iteration.Used to estimate initial move in 
the one-dimensional search. Updated as the optimization progresses. 
</TD><TD>1000</TD></TR>
			
<TR><TD><B>REDELXI </B></TD><TD></TD><TD>Maximum relative change in a design variable 
attempted on the first optimization iteration. Used to estimate the initial move 
in the one-dimensional search. Updated as the optimization progresses. 
</TD><TD>0.01</TD></TR>
			
<TR><TD><B>ABDELXI </B></TD><TD></TD><TD>Maximum absolute change in a design variable attempted 
on the first optimization iteration. Used to estimate the initial move in the one-dimensional 
search. Updated as the optimization progresses. </TD><TD>0.2</TD></TR>
			

<TR><TD><B>EXTRAP</B></TD><TD></TD><TD>Maximum multiplier of the search step in the 
one-dimensional search using polynomial interpolation/extrapolation. </TD><TD>5</TD></TR>
			

<TR><TD><B>SCALO</B></TD><TD></TD><TD>The user-supplied value of the scale factor for the 
objective function if the default or calculated value is to be over-ridden.</TD><TD>1.0</TD></TR>
			

<TR><TD><B>SCALMIN</B></TD><TD></TD><TD>Minimum numerical value of any scale factor allowed.</TD><TD>0.001</TD></TR>
			

<TR><TD><B>CTOL </B></TD><TD></TD><TD>Constraint tolerance in the Method of Feasible Directions or the 
Modified Method of Feasible Directions. A constraint is active if its numerical value is more 
positive than CTOL </TD><TD>-0.03</TD></TR>
			

<TR><TD><B>CLTOL</B></TD><TD></TD><TD>Same as CTOL but for linear constraints.</TD><TD>-0.005</TD></TR>

<TR><TD><B>CLTOLMIN </B></TD><TD></TD><TD>Same as CTOLMIN, but for linear constraints.</TD><TD>0.001</TD></TR>

<TR><TD><B>CTOLMIN </B></TD><TD></TD><TD>Minimum constraint tolerance for nonlinear constraints. If a constraint 
is more positive than CTOLMIN, it is considered to be violated. 
</TD><TD>0.01</TD></TR>
			
<TR><TD><B>RIMOVE </B></TD><TD></TD><TD>Initial relative move limit. Used to set the move limits in 
sequential linear programming, method of inscribed hyperspheres and sequential quadratic programming 
as a fraction of the value of the unknowns. </TD><TD>0.2</TD></TR>
			

<TR><TD><B>STOL</B></TD><TD></TD><TD>Tolerance on the components of the calculated 
search direction to indicate that the Kuhn-Tucker conditions are satisfied. 
</TD><TD>0.001</TD></TR>
			
<TR><TD><B>PUSHOFF</B></TD><TD></TD><TD>Nominal value of the push-off factor Method of Feasible Directions. 
</TD><TD>0.1</TD></TR>
			
<TR><TD><B>VMULT</B></TD><TD></TD><TD>Multiplier on the search step in the one-dimensional 
search to find bounds on the solution.</TD><TD>2.618034</TD></TR>
			

<TR><TD><B>ZRO</B></TD><TD></TD><TD>Numerical estimate of zero on the computer. 
Usually the default value is adequate. If a computer with a short word length is used, 
ZRO=10E-4 may be preferred.</TD><TD>0.00001</TD></TR>
</TABLE>
<HR>			

